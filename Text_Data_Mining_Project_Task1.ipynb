{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This project will explore and analyze the information stored in a particular dataset. In this case, the ACL Anthology dataset (https://aclanthology.org/). We will explore different techniques for obtaining valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Finding Similar Items\n",
    " \n",
    "Randomly select 1000 abstracts from the whole dataset. Find the similar items using pairwise Jaccard similarities, MinHash and LSH (vectorized versions) .\n",
    "\n",
    "1. Compare the performance in time and the results for k-shingles = 3, 5 and 10, for the three methods and similarity thresholds s=0.1 and 0.2. Use 50 hashing functions. Comment your results.\n",
    "\n",
    "2. Compare the results obtained for MinHash and LSH for different similarity thresholds s = 0.1, 0.2 and 0.25 and 50, 100 and 200 hashing functions. Comment your results.\n",
    "\n",
    "3. For MinHashing using 100 hashing functions and s = 0.1 and 0.2, find the Jaccard distances (1-Jaccard similarity) for all possible pairs. Use the obtained values within a k-NN algorithm, and for k=1,3 and, 5 identify the clusters with similar abstracts for each s. Describe the obtained clusters, are they different?. Select randomly at least 5 abstracts per cluster, upon visual inspection, what are the main topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "\n",
    "#from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import binascii\n",
    "from time import time\n",
    "\n",
    "from urllib import request\n",
    "import gzip\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract dataset\n",
    "\n",
    "url1 = \"https://aclanthology.org/anthology+abstracts.bib.gz\"\n",
    "file_name1 = re.split(pattern='/', string=url1)[-1]\n",
    "r1 = request.urlretrieve(url=url1, filename=file_name1)\n",
    "txt1 = re.split(pattern=r'\\.', string=file_name1)[0] + \".txt\"\n",
    "\n",
    "# Extract it\n",
    "with gzip.open(file_name1, 'rb') as f_in:\n",
    "    with open(txt1, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "fname = txt1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various Functions\n",
    "#=================================================#\n",
    "#=================================================#\n",
    "#=================================================#\n",
    "\n",
    "# abstract extracting function\n",
    "# fname is the file name of the document containing all abstract\n",
    "# n is the number of abstracts that we will extract\n",
    "def read_abstracts(fname,n):\n",
    "    abs = [] #initialize a list variable\n",
    "    with open(fname, 'r', encoding=\"utf-8\") as f:\n",
    "        i = 0\n",
    "        # skip all lines until abstract\n",
    "        for line in f:\n",
    "            if \"abstract =\" in line:\n",
    "                pattern = '\"'\n",
    "                abstract = re.split(pattern ,line, flags=re.IGNORECASE)[1].split('\"')[0]\n",
    "                if len(abstract)<5: # takes care of empty abstracts\n",
    "                    pass\n",
    "                    \n",
    "                else:\n",
    "                    abs.append(abstract) # append each abstract to the list\n",
    "                    i = i + 1\n",
    "                if i == n:  # number of abstracts to extract\n",
    "                    return abs\n",
    "        \n",
    "        return abs\n",
    "#=================================================\n",
    "    \n",
    "# Shingle function\n",
    "# k is the number of shingles\n",
    "\n",
    "def get_shingles(abstract, k):\n",
    "    \"\"\"Get all shingles from requested file (hashes of these shingles)\n",
    "    \"\"\"\n",
    "    L = len(abstract)\n",
    "    shingles = set()  # we use a set to automatically eliminate duplicates\n",
    "    for i in range(L-k+1):\n",
    "        shingle = abstract[i:i+k]\n",
    "        crc = binascii.crc32(shingle.encode('utf-8')) # hash the shingle to a 32-bit integer\n",
    "        shingles.add(crc)\n",
    "    return shingles\n",
    "#=================================================\n",
    "\n",
    "# jaccard similarity score Function\n",
    "def jaccard_similarity_score(x, y, errors='ignore'):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity J (A,B) = | Intersection (A,B) | /\n",
    "                                    | Union (A,B) |\n",
    "    \"\"\"\n",
    "    intersection_cardinality = len(set(x).intersection(set(y)))\n",
    "    union_cardinality = len(set(x).union(set(y)))\n",
    "    if float(union_cardinality) == 0:\n",
    "        ja = 0\n",
    "    else:\n",
    "        ja = intersection_cardinality / float(union_cardinality)\n",
    "    return ja\n",
    "#=================================================\n",
    "\n",
    "# similarity functions\n",
    "# k is number of shingles and s is the similarity thresholds \n",
    "# abstract_list is the list of 1000 abstracts\n",
    "\n",
    "def similar_items(abstract_list, k, s):\n",
    "    candidates = []\n",
    "    #abstract_list = read_abstracts(fname,n)\n",
    "    for pair in itertools.combinations(abstract_list,2):\n",
    "        js = jaccard_similarity_score(get_shingles(pair[0], k),get_shingles(pair[1], k))\n",
    "        \n",
    "        if js > s:\n",
    "            #print(pair)\n",
    "            candidates.append(pair)\n",
    "            \n",
    "    return candidates\n",
    "#=================================================\n",
    "\n",
    "\n",
    "# fast implementation of Minhashing algorithm\n",
    "# computes all random hash functions for a shingle at once, using vector operations\n",
    "# also finds element-wise minimum of two vectors efficiently\n",
    "def minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig):\n",
    "    signature = numpy.ones((nsig,)) * (maxShingleID + 1)\n",
    "\n",
    "    for ShingleID in shingles:\n",
    "        hashCodes = ((A*ShingleID + B) % nextPrime) % maxShingleID\n",
    "        numpy.minimum(signature, hashCodes, out=signature)\n",
    "\n",
    "    return signature\n",
    "#=================================================\n",
    "\n",
    "# candidate pair function\n",
    "def candidate_pair(abstract_list, k, s):\n",
    "    signatures = []  # signatures for all files\n",
    "    for abstract in abstract_list:\n",
    "        shingles = get_shingles(abstract, k)\n",
    "        signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "        signatures.append(signature)\n",
    "        \n",
    "    Nfiles = len(signatures)\n",
    "    #startTime = time.time()\n",
    "    candidates = []\n",
    "    for i in range(Nfiles):\n",
    "        for j in range(i+1, Nfiles):\n",
    "            Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "            if Jsim >= s:                                      # two vectors, equivalente to Jaccard\n",
    "                candidates.append((i,j))\n",
    "                \n",
    "            \n",
    "    return len(candidates)\n",
    "#=================================================\n",
    "\n",
    "# Moditied Function for jaccard similarity\n",
    "\n",
    "def jaccard_similarity_score_mod2(a, b, shingles_list, errors='ignore'): \n",
    "    \n",
    "    sha = shingles_list[a]\n",
    "    shingles_vector_a = sha\n",
    " \n",
    "    shb = shingles_list[b]\n",
    "    shingles_vector_b = shb\n",
    "\n",
    "    jsc = jaccard_similarity_score(shingles_vector_a, shingles_vector_b)\n",
    "    \n",
    "    return jsc\n",
    "#=================================================\n",
    "\n",
    "# LSH candidates function\n",
    "def LSH(signatures, bands, rows, Ab, Bb, nextPrime, maxShingleID, s, shingles_list):\n",
    "    \"\"\"Locality Sensitive Hashing\n",
    "    \"\"\"\n",
    "    numItems = signatures.shape[1]\n",
    "    signBands = numpy.array_split(signatures, bands, axis=0) \n",
    "    candidates = set()\n",
    "    for nb in range(bands):\n",
    "        hashTable = {}\n",
    "        for ni in range(numItems):\n",
    "            item = signBands[nb][:,ni]\n",
    "            hash = (numpy.dot(Ab[nb,:], item) + Bb[nb]) % nextPrime % maxShingleID\n",
    "            if hash not in hashTable:\n",
    "                hashTable[hash] = [ni]\n",
    "            else:\n",
    "                hashTable[hash].append(ni)\n",
    "        for _,items in hashTable.items():\n",
    "            if len(items) > 1:\n",
    "                L = len(items)\n",
    "                for i in range(L-1):\n",
    "                    for j in range(i+1, L):\n",
    "                        cand = [items[i], items[j]]\n",
    "                        a = items[i]\n",
    "                        b = items[j]\n",
    "                        jsim = jaccard_similarity_score_mod2(a,b, shingles_list) #jaccard similarity function call\n",
    "                        if jsim >= s:\n",
    "                            numpy.sort(cand)\n",
    "                            candidates.add(tuple(cand))\n",
    "    return candidates\n",
    "#=================================================\n",
    "\n",
    "# LSH candidates length function\n",
    "def LSH_candidates(abstract_list, k, s):\n",
    "    signatures = []  # signatures for all files\n",
    "    shingles_list =[]\n",
    "    for abstract in abstract_list:\n",
    "        shingles = get_shingles(abstract, k)\n",
    "        signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "        signatures.append(signature)\n",
    "        shingles_list.append(shingles)\n",
    "        \n",
    "    \n",
    "    A2 = numpy.random.randint(0, nextPrime/2, size=(bands, rows),dtype=numpy.int64)  # now we need a vector of A parameters for each band\n",
    "    B2 = numpy.random.randint(0, nextPrime/2, size=(bands, ),dtype=numpy.int64)\n",
    "    signatures = numpy.array(signatures).T  # LSH needs a matrix of signatures, not a list of vectors\n",
    "\n",
    "  \n",
    "    candidates = LSH(signatures, bands, rows, A2, B2, nextPrime, maxShingleID, s, shingles_list)\n",
    "   \n",
    "    \n",
    "    return len(candidates)\n",
    "#=================================================\n",
    "\n",
    "# New minHash function for Task1 number2 a\n",
    "def Sim_Method_Property(abstract_list,k,s,bands,rows):\n",
    "    \n",
    "    nsig = bands*rows  # hashing function: number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "    #maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "    #nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "    #A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "    #B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "    \n",
    "    startTime = time.time()\n",
    "    cand = candidate_pair(abstract_list, k, s)\n",
    "    execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "    minHash_k.append(k)\n",
    "    minHash_s.append(s)\n",
    "    Hashing_fn.append(nsig)\n",
    "    minHash_sim.append(cand)\n",
    "    minHash_execT.append(execTime)\n",
    "    \n",
    "    dict = {'k': minHash_k, 's': minHash_s, 'Hashing_fn': Hashing_fn,'#sim': minHash_sim, 'execTime(sec)': minHash_execT} \n",
    "    df = pd.DataFrame(dict)\n",
    "    \n",
    "    return df\n",
    "#=================================================\n",
    "\n",
    "# New LSH function for Task1 number2 b\n",
    "def Sim_Method_Property2(abstract_list,k,s,bands,rows):\n",
    "    \n",
    "    \n",
    "    nsig = bands*rows  # hashing function: number of elements in signature, or the number of different random hash functions\n",
    "    \n",
    "    startTime = time.time()\n",
    "    candi = LSH_candidates(abstract_list,k, s)\n",
    "    execTime = round(((time.time() - startTime)),2)\n",
    "    \n",
    "    LSH_k.append(k)\n",
    "    LSH_s.append(s)\n",
    "    Hashing_fn2.append(nsig)\n",
    "    LSH_sim.append(candi)\n",
    "    LSH_execT.append(execTime)\n",
    "    \n",
    "    dict = {'k': LSH_k, 's': LSH_s, 'Hashing_fn': Hashing_fn2,'#sim': LSH_sim, 'execTime(sec)': LSH_execT} \n",
    "    df = pd.DataFrame(dict)\n",
    "    \n",
    "    return df\n",
    "#=================================================\n",
    "\n",
    "# functions for Task1 number 3\n",
    "# Jaccard distance calculator function\n",
    "\n",
    "def jacc_dist_calc(abstract_list,k,s,bands,rows):\n",
    "    \n",
    "    nsig = bands*rows  # hashing function: number of elements in signature, or the number of different random hash functions\n",
    "    \n",
    "    jd_df = candidate_pair_jacc_dist(abstract_list, k, s, nsig)\n",
    "\n",
    "    return jd_df\n",
    "#==============\n",
    "\n",
    "# A modified candidate pair function\n",
    "\n",
    "def candidate_pair_jacc_dist(abstract_list, k, s, nsig):\n",
    "    signatures = []  # signatures for all files\n",
    "    shingles_list = []\n",
    "    for abstract in abstract_list:\n",
    "        shingles = get_shingles(abstract, k)\n",
    "        signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "        signatures.append(signature)\n",
    "        shingles_list.append(shingles)\n",
    "        \n",
    "    Nfiles = len(signatures)\n",
    "    candidates = []\n",
    "    jaccard_distance = []\n",
    "    s_list = []\n",
    "    #k_list = []\n",
    "    #h_fn_list = []\n",
    "    #sign1 = []\n",
    "    #sign2 = []\n",
    "    for i in range(Nfiles):\n",
    "        for j in range(i+1, Nfiles):\n",
    "            Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "            if Jsim >= s:                                      # two vectors, equivalente to Jaccard\n",
    "                #a = i\n",
    "                #b = j\n",
    "                js = jaccard_similarity_score_mod2(i,j, shingles_list)\n",
    "                jaccard_distance.append(1-js) # jaccard distance calculations\n",
    "                s_list.append(s)\n",
    "                #k_list.append(k)\n",
    "                #h_fn_list.append(nsig)\n",
    "                candidates.append((i,j))\n",
    "                #sign1.append(signatures[i])\n",
    "                #sign2.append(signatures[j])\n",
    "    \n",
    "    dict = {'s': s_list, 'candidates': candidates, 'jacc_distance': jaccard_distance} \n",
    "    df = pd.DataFrame(dict)\n",
    "    return df    #len(candidates)\n",
    "#=================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25058823529411767"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let run a test as we extract 1000 abstract and using k=3\n",
    "abstract_list = read_abstracts(fname,1000) \n",
    "shingles_vectors = []\n",
    "\n",
    "for item in abstract_list[:1000]: \n",
    "    sh = list(get_shingles(item, 3))\n",
    "    shingles_vectors.append(sh)\n",
    "    \n",
    "\n",
    "# the first two abstracts\n",
    "jaccard_similarity_score(shingles_vectors[0], shingles_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# samples size\n",
    "sample_size = numpy.array(abstract_list).T\n",
    "sample_size.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task1 Number 1\n",
    "\n",
    "Compare the performance in time and the results for k-shingles = 3, 5 and 10, for the three methods and similarity thresholds s=0.1 and 0.2. Use 50 hashing functions. Comment your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) Finding similar items using Pairwise Jaccard similarities\n",
    "each cells will take some minutes to spin up because of the volume of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing collector list\n",
    "similarity_k = []\n",
    "similarity_s = []\n",
    "similarity_sim = []\n",
    "similarity_execT = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=3\n",
    "- similarity thresholds, s=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "s = 0.1\n",
    "\n",
    "startTime = time.time()\n",
    "sim = len(similar_items(abstract_list,k,s))\n",
    "execTime = round(((time.time() - startTime)/60),2)\n",
    "\n",
    "similarity_k.append(k)\n",
    "similarity_s.append(s)\n",
    "similarity_sim.append(sim)\n",
    "similarity_execT.append(execTime)\n",
    "\n",
    "#print(\"Number of similar items: {}\".format(sim))\n",
    "#print(\"Execution time:  {}\".format(execTime) + \" mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=3\n",
    "- similarity thresholds, s=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "s = 0.2\n",
    "\n",
    "startTime = time.time()\n",
    "sim = len(similar_items(abstract_list,k,s))\n",
    "execTime = round(((time.time() - startTime)/60),2)\n",
    "\n",
    "similarity_k.append(k)\n",
    "similarity_s.append(s)\n",
    "similarity_sim.append(sim)\n",
    "similarity_execT.append(execTime)\n",
    "\n",
    "#print(\"Number of similar items: {}\".format(sim))\n",
    "#print(\"Execution time:  {}\".format(execTime) + \" mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=5\n",
    "- similarity thresholds, s=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "s = 0.1\n",
    "\n",
    "startTime = time.time()\n",
    "sim = len(similar_items(abstract_list,k,s))\n",
    "execTime = round(((time.time() - startTime)/60),2)\n",
    "\n",
    "similarity_k.append(k)\n",
    "similarity_s.append(s)\n",
    "similarity_sim.append(sim)\n",
    "similarity_execT.append(execTime)\n",
    "\n",
    "#print(\"Number of similar items: {}\".format(sim))\n",
    "#print(\"Execution time:  {}\".format(execTime) + \" mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=5\n",
    "- similarity thresholds, s=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "s = 0.2\n",
    "\n",
    "startTime = time.time()\n",
    "sim = len(similar_items(abstract_list,k,s))\n",
    "execTime = round(((time.time() - startTime)/60),2)\n",
    "\n",
    "similarity_k.append(k)\n",
    "similarity_s.append(s)\n",
    "similarity_sim.append(sim)\n",
    "similarity_execT.append(execTime)\n",
    "\n",
    "#print(\"Number of similar items: {}\".format(sim))\n",
    "#print(\"Execution time:  {}\".format(execTime) + \" mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=10\n",
    "- similarity thresholds, s=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "s = 0.1\n",
    "\n",
    "startTime = time.time()\n",
    "sim = len(similar_items(abstract_list,k,s))\n",
    "execTime = round(((time.time() - startTime)/60),2)\n",
    "\n",
    "similarity_k.append(k)\n",
    "similarity_s.append(s)\n",
    "similarity_sim.append(sim)\n",
    "similarity_execT.append(execTime)\n",
    "\n",
    "#print(\"Number of similar items: {}\".format(sim))\n",
    "#print(\"Execution time:  {}\".format(execTime) + \" mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=10\n",
    "- similarity thresholds, s=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "s = 0.2\n",
    "\n",
    "startTime = time.time()\n",
    "sim = len(similar_items(abstract_list,k,s))\n",
    "execTime = round(((time.time() - startTime)/60),2)\n",
    "\n",
    "similarity_k.append(k)\n",
    "similarity_s.append(s)\n",
    "similarity_sim.append(sim)\n",
    "similarity_execT.append(execTime)\n",
    "\n",
    "#print(\"Number of similar items: {}\".format(sim))\n",
    "#print(\"Execution time:  {}\".format(execTime) + \" mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'k': similarity_k, 's': similarity_s, '#sim': similarity_sim, 'execTime(min)': similarity_execT} \n",
    "df = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) Finding similar items using MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "minHash_k = []\n",
    "minHash_s = []\n",
    "minHash_sim = []\n",
    "minHash_execT = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters to process the whole dataset\n",
    "bands = 10\n",
    "rows = 5\n",
    "nsig = bands*rows  #50 hashing function: number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=3\n",
    "- similarity thresholds, s=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "s = 0.1\n",
    "\n",
    "startTime = time.time()\n",
    "candi = candidate_pair(abstract_list, k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "minHash_k.append(k)\n",
    "minHash_s.append(s)\n",
    "minHash_sim.append(candi)\n",
    "minHash_execT.append(execTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=3\n",
    "- similarity thresholds, s=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "s = 0.2\n",
    "\n",
    "startTime = time.time()\n",
    "candi = candidate_pair(abstract_list, k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "minHash_k.append(k)\n",
    "minHash_s.append(s)\n",
    "minHash_sim.append(candi)\n",
    "minHash_execT.append(execTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=5\n",
    "- similarity thresholds, s=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "s = 0.1\n",
    "\n",
    "startTime = time.time()\n",
    "cand = candidate_pair(abstract_list, k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "minHash_k.append(k)\n",
    "minHash_s.append(s)\n",
    "minHash_sim.append(cand)\n",
    "minHash_execT.append(execTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=5\n",
    "- similarity thresholds, s=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "s = 0.2\n",
    "\n",
    "startTime = time.time()\n",
    "cand = candidate_pair(abstract_list, k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "minHash_k.append(k)\n",
    "minHash_s.append(s)\n",
    "minHash_sim.append(cand)\n",
    "minHash_execT.append(execTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=10\n",
    "- similarity thresholds, s=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "s = 0.1\n",
    "\n",
    "startTime = time.time()\n",
    "cand = candidate_pair(abstract_list, k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "minHash_k.append(k)\n",
    "minHash_s.append(s)\n",
    "minHash_sim.append(cand)\n",
    "minHash_execT.append(execTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=10\n",
    "- similarity thresholds, s=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "s = 0.2\n",
    "\n",
    "startTime = time.time()\n",
    "cand = candidate_pair(abstract_list, k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "minHash_k.append(k)\n",
    "minHash_s.append(s)\n",
    "minHash_sim.append(cand)\n",
    "minHash_execT.append(execTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2 = {'k': minHash_k, 's': minHash_s, '#sim': minHash_sim, 'execTime(sec)': minHash_execT} \n",
    "df2 = pd.DataFrame(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (c) Finding similar items using Locality-Sensitive Hashing ( LSH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing collector list\n",
    "LSH_k = []\n",
    "LSH_s = []\n",
    "LSH_sim = []\n",
    "LSH_execT = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=3\n",
    "- similarity thresholds, s=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "s = 0.1\n",
    "\n",
    "startTime = time.time()\n",
    "candi = LSH_candidates(abstract_list,k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "LSH_k.append(k)\n",
    "LSH_s.append(s)\n",
    "LSH_sim.append(candi)\n",
    "LSH_execT.append(execTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=3\n",
    "- similarity thresholds, s=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "s = 0.2\n",
    "\n",
    "startTime = time.time()\n",
    "candi = LSH_candidates(abstract_list,k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "LSH_k.append(k)\n",
    "LSH_s.append(s)\n",
    "LSH_sim.append(candi)\n",
    "LSH_execT.append(execTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=5\n",
    "- similarity thresholds, s=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "s = 0.1\n",
    "\n",
    "startTime = time.time()\n",
    "candi = LSH_candidates(abstract_list,k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "LSH_k.append(k)\n",
    "LSH_s.append(s)\n",
    "LSH_sim.append(candi)\n",
    "LSH_execT.append(execTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=5\n",
    "- similarity thresholds, s=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "s = 0.2\n",
    "\n",
    "startTime = time.time()\n",
    "candi = LSH_candidates(abstract_list,k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "LSH_k.append(k)\n",
    "LSH_s.append(s)\n",
    "LSH_sim.append(candi)\n",
    "LSH_execT.append(execTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=10\n",
    "- similarity thresholds, s=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "s = 0.1\n",
    "\n",
    "startTime = time.time()\n",
    "candi = LSH_candidates(abstract_list,k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "LSH_k.append(k)\n",
    "LSH_s.append(s)\n",
    "LSH_sim.append(candi)\n",
    "LSH_execT.append(execTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-shingles, k=10\n",
    "- similarity thresholds, s=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "s = 0.2\n",
    "\n",
    "startTime = time.time()\n",
    "candi = LSH_candidates(abstract_list,k, s)\n",
    "execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "LSH_k.append(k)\n",
    "LSH_s.append(s)\n",
    "LSH_sim.append(candi)\n",
    "LSH_execT.append(execTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict3 = {'k': LSH_k, 's': LSH_s, '#sim': LSH_sim, 'execTime(sec)': LSH_execT} \n",
    "df3 = pd.DataFrame(dict3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Number 1 - results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairwise jaccard_similarity results\n",
      "    k    s    #sim  execTime(min)\n",
      "0   3  0.1  496590          10.13\n",
      "1   3  0.2  285927           9.69\n",
      "2   5  0.1    5308           9.72\n",
      "3   5  0.2      24          10.34\n",
      "4  10  0.1      22          10.61\n",
      "5  10  0.2       7          10.98\n",
      "\n",
      "minHash results\n",
      "    k    s    #sim  execTime(sec)\n",
      "0   3  0.1  470846          14.40\n",
      "1   3  0.2  224575          13.41\n",
      "2   5  0.1  149250          14.94\n",
      "3   5  0.2    2608          13.92\n",
      "4  10  0.1     422          15.35\n",
      "5  10  0.2      13          13.90\n",
      "\n",
      "LSH results\n",
      "    k    s  #sim  execTime(sec)\n",
      "0   3  0.1  2761           6.76\n",
      "1   3  0.2  2268           6.17\n",
      "2   5  0.1     3           8.32\n",
      "3   5  0.2     2           7.44\n",
      "4  10  0.1     1           8.91\n",
      "5  10  0.2     1           8.88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('pairwise jaccard_similarity results'); print(df); print(''); print('minHash results'); print(df2); print(''); print('LSH results'); print(df3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENT\n",
    "\n",
    "- MinHash is supposed to be faster than pairwsie jaccard similarity but its clearly not here, but LSH is clearly the fastest\n",
    "- As the shingle value, k and similarity threshold value, s increases the number of similar items reduces\n",
    "- But certainly we see a trade-off; as the method enable the process to get faster, we get lesser similar items (i.e. we get fewer results for similar abstracts, its like some are skipped or thrown away or some comparisms are not made)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task1 Number 2\n",
    "\n",
    "Compare the results obtained for MinHash and LSH for different similarity thresholds s = 0.1, 0.2 and 0.25 and 50, 100 and 200 hashing functions. Comment your results.\n",
    "\n",
    "We are going to use k_shingles 3 and 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) Finding similar items using MinHash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### K_shingles = 3 and all the other parameters\n",
    "#================================================\n",
    "\n",
    "# initializing the collector list\n",
    "minHash_k = []\n",
    "minHash_s = []\n",
    "Hashing_fn = []\n",
    "minHash_sim = []\n",
    "minHash_execT = []\n",
    "\n",
    "# hashing function 50\n",
    "Sim_Method_Property(abstract_list,k=3,s=0.1,bands=10,rows=5)\n",
    "Sim_Method_Property(abstract_list,k=3,s=0.2,bands=10,rows=5)\n",
    "Sim_Method_Property(abstract_list,k=3,s=0.25,bands=10,rows=5)\n",
    "\n",
    "# hashing function 100\n",
    "Sim_Method_Property(abstract_list,k=3,s=0.1,bands=20,rows=5)\n",
    "Sim_Method_Property(abstract_list,k=3,s=0.2,bands=20,rows=5)\n",
    "Sim_Method_Property(abstract_list,k=3,s=0.25,bands=20,rows=5)\n",
    "\n",
    "# hashing function 200\n",
    "Sim_Method_Property(abstract_list,k=3,s=0.1,bands=40,rows=5)\n",
    "Sim_Method_Property(abstract_list,k=3,s=0.2,bands=40,rows=5)\n",
    "\n",
    "df_21_k3 = Sim_Method_Property(abstract_list,k=3,s=0.25,bands=40,rows=5)\n",
    "\n",
    "\n",
    "#### K_shingles = 5 and all the other parameters\n",
    "#===============================================\n",
    "\n",
    "# initializing the collector list\n",
    "minHash_k = []\n",
    "minHash_s = []\n",
    "Hashing_fn = []\n",
    "minHash_sim = []\n",
    "minHash_execT = []\n",
    "\n",
    "# hashing function 50\n",
    "Sim_Method_Property(abstract_list,k=5,s=0.1,bands=10,rows=5)\n",
    "Sim_Method_Property(abstract_list,k=5,s=0.2,bands=10,rows=5)\n",
    "Sim_Method_Property(abstract_list,k=5,s=0.25,bands=10,rows=5)\n",
    "\n",
    "# hashing function 100\n",
    "Sim_Method_Property(abstract_list,k=5,s=0.1,bands=20,rows=5)\n",
    "Sim_Method_Property(abstract_list,k=5,s=0.2,bands=20,rows=5)\n",
    "Sim_Method_Property(abstract_list,k=5,s=0.25,bands=20,rows=5)\n",
    "\n",
    "# hashing function 200\n",
    "Sim_Method_Property(abstract_list,k=5,s=0.1,bands=40,rows=5)\n",
    "Sim_Method_Property(abstract_list,k=5,s=0.2,bands=40,rows=5)\n",
    "\n",
    "df_21_k5 = Sim_Method_Property(abstract_list,k=5,s=0.25,bands=40,rows=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) Finding similar items using LSH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### K_shingles = 3 and all the other parameters\n",
    "#================================================\n",
    "\n",
    "# initializing the collector list\n",
    "\n",
    "LSH_k = []\n",
    "LSH_s = []\n",
    "Hashing_fn2 = []\n",
    "LSH_sim = []\n",
    "LSH_execT = []\n",
    "\n",
    "# hashing function 50\n",
    "Sim_Method_Property2(abstract_list,k=3,s=0.1,bands=10,rows=5)\n",
    "Sim_Method_Property2(abstract_list,k=3,s=0.2,bands=10,rows=5)\n",
    "Sim_Method_Property2(abstract_list,k=3,s=0.25,bands=10,rows=5)\n",
    "\n",
    "# hashing function 100\n",
    "Sim_Method_Property2(abstract_list,k=3,s=0.1,bands=20,rows=5)\n",
    "Sim_Method_Property2(abstract_list,k=3,s=0.2,bands=20,rows=5)\n",
    "Sim_Method_Property2(abstract_list,k=3,s=0.25,bands=20,rows=5)\n",
    "\n",
    "# hashing function 200\n",
    "Sim_Method_Property2(abstract_list,k=3,s=0.1,bands=40,rows=5)\n",
    "Sim_Method_Property2(abstract_list,k=3,s=0.2,bands=40,rows=5)\n",
    "\n",
    "df_22_k3 = Sim_Method_Property2(abstract_list,k=3,s=0.25,bands=40,rows=5)\n",
    "\n",
    "\n",
    "\n",
    "#### K_shingles = 5 and all the other parameters\n",
    "#===============================================\n",
    "\n",
    "# initializing the collector list\n",
    "\n",
    "LSH_k = []\n",
    "LSH_s = []\n",
    "Hashing_fn2 = []\n",
    "LSH_sim = []\n",
    "LSH_execT = []\n",
    "\n",
    "# hashing function 50\n",
    "Sim_Method_Property2(abstract_list,k=5,s=0.1,bands=10,rows=5)\n",
    "Sim_Method_Property2(abstract_list,k=5,s=0.2,bands=10,rows=5)\n",
    "Sim_Method_Property2(abstract_list,k=5,s=0.25,bands=10,rows=5)\n",
    "\n",
    "# hashing function 100\n",
    "Sim_Method_Property2(abstract_list,k=5,s=0.1,bands=20,rows=5)\n",
    "Sim_Method_Property2(abstract_list,k=5,s=0.2,bands=20,rows=5)\n",
    "Sim_Method_Property2(abstract_list,k=5,s=0.25,bands=20,rows=5)\n",
    "\n",
    "# hashing function 200\n",
    "Sim_Method_Property2(abstract_list,k=5,s=0.1,bands=40,rows=5)\n",
    "Sim_Method_Property2(abstract_list,k=5,s=0.2,bands=40,rows=5)\n",
    "\n",
    "df_22_k5 = Sim_Method_Property2(abstract_list,k=5,s=0.25,bands=40,rows=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number 2 - results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minhash K= 3\n",
      "   k     s  Hashing_fn    #sim  execTime(sec)\n",
      "0  3  0.10          50  470846          13.23\n",
      "1  3  0.20          50  224575          13.04\n",
      "2  3  0.25          50   72679          12.43\n",
      "3  3  0.10         100  470846          14.09\n",
      "4  3  0.20         100  224575          13.77\n",
      "5  3  0.25         100   72679          11.87\n",
      "6  3  0.10         200  470846          14.02\n",
      "7  3  0.20         200  224575          13.55\n",
      "8  3  0.25         200   72679          12.22\n",
      "\n",
      "minHash k = 5\n",
      "   k     s  Hashing_fn    #sim  execTime(sec)\n",
      "0  5  0.10          50  149250          13.94\n",
      "1  5  0.20          50    2608          13.13\n",
      "2  5  0.25          50     120          13.61\n",
      "3  5  0.10         100  149250          14.17\n",
      "4  5  0.20         100    2608          14.07\n",
      "5  5  0.25         100     120          13.21\n",
      "6  5  0.10         200  149250          14.77\n",
      "7  5  0.20         200    2608          14.23\n",
      "8  5  0.25         200     120          14.29\n",
      "==========================================\n",
      "LSH K= 3\n",
      "   k     s  Hashing_fn  #sim  execTime(sec)\n",
      "0  3  0.10          50  2761           9.05\n",
      "1  3  0.20          50  2268           7.34\n",
      "2  3  0.25          50   815           6.76\n",
      "3  3  0.10         100  2761           6.42\n",
      "4  3  0.20         100  2268           6.60\n",
      "5  3  0.25         100   815           6.44\n",
      "6  3  0.10         200  2761           6.45\n",
      "7  3  0.20         200  2268           6.07\n",
      "8  3  0.25         200   815           6.11\n",
      "\n",
      "LSH k = 5\n",
      "   k     s  Hashing_fn  #sim  execTime(sec)\n",
      "0  5  0.10          50     3           7.57\n",
      "1  5  0.20          50     2           7.26\n",
      "2  5  0.25          50     2           7.95\n",
      "3  5  0.10         100     3           7.78\n",
      "4  5  0.20         100     2          10.50\n",
      "5  5  0.25         100     2           7.60\n",
      "6  5  0.10         200     3           7.54\n",
      "7  5  0.20         200     2           7.44\n",
      "8  5  0.25         200     2           7.81\n"
     ]
    }
   ],
   "source": [
    "# RESULTS\n",
    "\n",
    "print('Minhash K= 3'); print(df_21_k3); print(''); print('minHash k = 5'); print(df_21_k5)\n",
    "print(\"==========================================\")\n",
    "print('LSH K= 3'); print(df_22_k3); print(''); print('LSH k = 5'); print(df_22_k5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENT\n",
    "\n",
    "- First of all it is obvious that LSH is faster than minHashing\n",
    "- Secondly, there is a trade-off, because of speed as we have fewer simmilarities for LSH for the same set of parameters as in minHashing\n",
    "- Thirdly, for a given shingle number K we see that the number of similar abstracts are the same across board for equal similarity_thresholds s, irrespective of the hashing function size\n",
    "- And lastly, we notice that the execution time increases marginally with the number of hashing functions used.\n",
    "\n",
    "It therefore follows that we can use a lower optimum number of hashing fuction say 50 to get a good result combine with efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task1 Number 3\n",
    "For MinHashing using 100 hashing functions and s = 0.1 and 0.2, find the Jaccard distances (1-Jaccard similarity) for all possible pairs. Use the obtained values within a k-NN algorithm, and for k=1,3 and, 5 identify the clusters with similar abstracts for each s. Describe the obtained clusters, are they different?. Select randomly at least 5 abstracts per cluster, upon visual inspection, what are the main topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = 20\n",
    "rows = 5\n",
    "nsig = bands*rows\n",
    "A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "s01 = jacc_dist_calc(abstract_list,k=3,s=0.1,bands=bands,rows=rows)\n",
    "s02 = jacc_dist_calc(abstract_list,k=3,s=0.2,bands=bands,rows=rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>candidates</th>\n",
       "      <th>jacc_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>0.749412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>0.772559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>0.772973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     s candidates  jacc_distance\n",
       "0  0.1     (0, 1)       0.749412\n",
       "1  0.1     (0, 2)       0.772559\n",
       "2  0.1     (0, 3)       0.772973"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s01.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = s01[['jacc_distance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for similarity threshhold, s = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[441682]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[395874]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[277707]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    indices\n",
       "0  [441682]\n",
       "1  [395874]\n",
       "2  [277707]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Nearest Neighbor, k = 1\n",
    "k = 1\n",
    "\n",
    "# fitting the model\n",
    "knn_model = NearestNeighbors(n_neighbors = k, algorithm = 'auto').fit(input_data)\n",
    "distances, indices = knn_model.kneighbors(input_data)\n",
    "\n",
    "# looping over the indices to make a list of the clusters\n",
    "ind = []\n",
    "for item in indices:\n",
    "    ind.append(item)\n",
    "\n",
    "# making a dataframe of it\n",
    "di = {'indices': ind}\n",
    "dfi = pd.DataFrame(di)    \n",
    "dfi.head(3) #this has two pairs of candidates in a cluster    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper describes the Global Tone Communication Co., Ltd.{'}s submission of the WMT21 shared news translation task. We participate in six directions: English to/from Hausa, Hindi to/from Bengali and Zulu to/from Xhosa. Our submitted systems are unconstrained and focus on multilingual translation odel, backtranslation and forward-translation. We also apply rules and language model to filter monolingual, parallel sentences and synthetic sentences.\n",
      "========================================================\n",
      "The objective of subtask 2 of SemEval-2021 Task 6 is to identify techniques used together with the span(s) of text covered by each technique. This paper describes the system and model we developed for the task. We first propose a pipeline system to identify spans, then to classify the technique in the input sequence. But it severely suffers from handling the overlapping in nested span. Then we propose to formulize the task as a question answering task by MRC framework which achieves a better result compared to the pipeline method. Moreover, data augmentation and loss design techniques are also explored to alleviate the problem of data sparse and imbalance. Finally, we attain the 3rd place in the final evaluation phase.\n",
      "##############################################################################\n",
      "Detecting Sarcasm has never been easy for machines to process. In this work, we present our submission of the sub-task1 of the shared task on sarcasm and sentiment detection in Arabic organized by the 6th Workshop for Arabic Natural Language Processing. In this work, we explored different approaches based on BERT models. First, we fine-tuned the AraBERTv02 model for the sarcasm detection task. Then, we used the Sentence-BERT model trained with contrastive learning to extract representative tweet embeddings. Finally, inspired by how the human brain comprehends the surface and the implicit meanings of sarcastic tweets, we combined the sentence embedding with the fine-tuned AraBERTv02 to further boost the performance of the model. Through the ensemble of the two models, our team ranked 5th out of 27 teams on the shared task of sarcasm detection in Arabic, with an F1-score of {\\%}59.89 on the official test data. The obtained result is {\\%}2.36 lower than the 1st place which confirms the capabilities of the employed combined model in detecting sarcasm.\n",
      "========================================================\n",
      "Automatic language identification is a challenging problem. Discriminating between closely related languages is especially difficult. This paper presents a machine learning approach for automatic language identification for the Nordic languages, which often suffer miscategorisation by existing state-of-the-art tools. Concretely we will focus on discrimination between six Nordic languages: Danish, Swedish, Norwegian (Nynorsk), Norwegian (Bokm{\\aa}l), Faroese and Icelandic.\n"
     ]
    }
   ],
   "source": [
    "###  The equal sign separate abstracts in the same cluster but the hash-tagg sign separates clusters\n",
    "\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][1589][0]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][1589][0]][1]])\n",
    "print('##############################################################################')\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][3579][0]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][3579][0]][1]])\n",
    "#print('##############################################################################')\n",
    "#print(abstract_list[s01['candidates'][dfi['indices'][345876][0]][0]])\n",
    "#print('========================================================')\n",
    "#print(abstract_list[s01['candidates'][dfi['indices'][345876][0]][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: \n",
    "- k=1 nearest neighbor and for s=0.1, yields clusters of just the candidate pairs as shown above\n",
    "- The first pair is about language translation\n",
    "- The second pair is about dialogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[22478, 441682, 428193]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[450667, 395874, 195403]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[216492, 277707, 11131]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    indices\n",
       "0   [22478, 441682, 428193]\n",
       "1  [450667, 395874, 195403]\n",
       "2   [216492, 277707, 11131]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Nearest Neighbor, k = 3\n",
    "k = 3\n",
    "knn_model = NearestNeighbors(n_neighbors = k, algorithm = 'auto').fit(input_data)\n",
    "distances, indices = knn_model.kneighbors(input_data)\n",
    "\n",
    "# looping over the indices to make a list of the clusters\n",
    "ind = []\n",
    "for item in indices:\n",
    "    ind.append(item)\n",
    "\n",
    "di = {'indices': ind}\n",
    "dfi = pd.DataFrame(di)    \n",
    "dfi.head(3)    #The indices shows the clusters we have in the dataframe s01\n",
    "              # Those set of numbers on each rows represents the indices on s01\n",
    "              # corresponding to the candidates pairs. That means in s01 (the\n",
    "              # dataframe with s=0.1 the candidate pairs at the indexes 22424, 0, 438)\n",
    "              # are in a cluster; that means their abstracts a similar. This cluster\n",
    "              # contains 6 abstracts (since each index has a pair of candidate similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this paper, we describe our approaches for task six of Social Media Mining for Health Applications (SMM4H) shared task in 2021. The task is to classify twitter tweets containing COVID-19 symptoms in three classes (self-reports, non-personal reports {\\&} literature/news mentions). We implemented BERT and XLNet for this text classification task. Best result was achieved by XLNet approach, which is F1 score 0.94, precision 0.9448 and recall 0.94448. This is slightly better than the average score, i.e. F1 score 0.93, precision 0.93235 and recall 0.93235.\n",
      "========================================================\n",
      "The upsurge of prolific blogging and microblogging platforms enabled the abusers to spread negativity and threats greater than ever. Detecting the toxic portions substantially aids to moderate or exclude the abusive parts for maintaining sound online platforms. This paper describes our participation in the SemEval 2021 toxic span detection task. The task requires detecting spans that convey toxic remarks from the given text. We explore an ensemble of sequence labeling models including the BiLSTM-CRF, spaCy NER model with custom toxic tags, and fine-tuned BERT model to identify the toxic spans. Finally, a majority voting ensemble method is used to determine the unified toxic spans. Experimental results depict the competitive performance of our model among the participants.\n",
      "========================================================\n",
      "Automatic Speech Recognition (ASR) systems generally do not produce punctuated transcripts. To make transcripts more readable and follow the expected input format for downstream language models, it is necessary to add punctuation marks. In this paper, we tackle the punctuation restoration problem specifically for the noisy text (e.g., phone conversation scenarios). To leverage the available written text datasets, we introduce a data sampling technique based on an n-gram language model to sample more training data that are similar to our in-domain data. Moreover, we propose a two-stage fine-tuning approach that utilizes the sampled external data as well as our in-domain dataset for models based on BERT. Extensive experiments show that the proposed approach outperforms the baseline with an improvement of 1.12{\\%} F1 score.\n",
      "========================================================\n",
      "We describe TelU-KU models of large-scale multilingual machine translation for five Southeast Asian languages: Javanese, Indonesian, Malay, Tagalog, Tamil, and English. We explore a variation of hyperparameters of flores101{\\_}mm100{\\_}175M model using random search with 10{\\%} of datasets to improve BLEU scores of all thirty language pairs. We submitted two models, TelU-KU-175M and TelU-KU- 175M{\\_}HPO, with average BLEU scores of 12.46 and 13.19, respectively. Our models show improvement in most language pairs after optimizing the hyperparameters. We also identified three language pairs that obtained a BLEU score of more than 15 while using less than 70 sentences of the training dataset: Indonesian-Tagalog, Tagalog-Indonesian, and Malay-Tagalog.\n",
      "========================================================\n",
      "Source-free domain adaptation is an emerging line of work in deep learning research since it is closely related to the real-world environment. We study the domain adaption in the sequence labeling problem where the model trained on the source domain data is given. We propose two methods: Self-Adapter and Selective Classifier Training. Self-Adapter is a training method that uses sentence-level pseudo-labels filtered by the self-entropy threshold to provide supervision to the whole model. Selective Classifier Training uses token-level pseudo-labels and supervises only the classification layer of the model. The proposed methods are evaluated on data provided by SemEval-2021 task 10 and Self-Adapter achieves 2nd rank performance.\n"
     ]
    }
   ],
   "source": [
    "print(abstract_list[s01['candidates'][dfi['indices'][412652][0]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][412652][0]][1]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][412652][1]][0]])\n",
    "#print('========================================================')\n",
    "#print(abstract_list[s01['candidates'][dfi['indices'][412652][1]][1]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][412652][2]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][412652][2]][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "k=3 nearest neighbors and for s=0.1, yields clusters of 6 abstracts i.e. 3 candidate pairs each\n",
    "\n",
    "- This cluster is about Text translation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[428193, 0, 479697, 441682, 22478]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[195403, 455819, 433467, 395874, 450667]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[11131, 405813, 216492, 277707, 404999]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    indices\n",
       "0        [428193, 0, 479697, 441682, 22478]\n",
       "1  [195403, 455819, 433467, 395874, 450667]\n",
       "2   [11131, 405813, 216492, 277707, 404999]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Nearest Neighbor, k = 5\n",
    "k = 5\n",
    "knn_model = NearestNeighbors(n_neighbors = k, algorithm = 'auto').fit(input_data)\n",
    "distances, indices = knn_model.kneighbors(input_data)\n",
    "\n",
    "# looping over the indices to make a list of the clusters\n",
    "ind = []\n",
    "for item in indices:\n",
    "    ind.append(item)\n",
    "\n",
    "di = {'indices': ind}\n",
    "dfi = pd.DataFrame(di)    \n",
    "dfi.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this paper, we describe our system submitted to SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense. The task aims at predicting whether the given text is humorous, the average humor rating given by the annotators, and whether the humor rating is controversial. In addition, the task also involves predicting how offensive the text is. Our approach adopts the DeBERTa architecture with disentangled attention mechanism, where the attention scores between words are calculated based on their content vectors and relative position vectors. We also took advantage of the pre-trained language models and fine-tuned the DeBERTa model on all the four subtasks. We experimented with several BERT-like structures and found that the large DeBERTa model generally performs better. During the evaluation phase, our system achieved an F-score of 0.9480 on subtask 1a, an RMSE of 0.5510 on subtask 1b, an F-score of 0.4764 on subtask 1c, and an RMSE of 0.4230 on subtask 2a (rank 3 on the leaderboard).\n",
      "========================================================\n",
      "Data sharing restrictions are common in NLP datasets. The purpose of this task is to develop a model trained in a source domain to make predictions for a target domain with related domain data. To address the issue, the organizers provided the models that fine-tuned a large number of source domain data on pre-trained models and the dev data for participants. But the source domain data was not distributed. This paper describes the provided model to the NER (Name entity recognition) task and the ways to develop the model. As a little data provided, pre-trained models are suitable to solve the cross-domain tasks. The models fine-tuned by large number of another domain could be effective in new domain because the task had no change.\n",
      "========================================================\n",
      "This paper describes the HEL-LJU submissions to the MultiLexNorm shared task on multilingual lexical normalization. Our system is based on a BERT token classification preprocessing step, where for each token the type of the necessary transformation is predicted (none, uppercase, lowercase, capitalize, modify), and a character-level SMT step where the text is translated from original to normalized given the BERT-predicted transformation constraints. For some languages, depending on the results on development data, the training data was extended by back-translating OpenSubtitles data. In the final ordering of the ten participating teams, the HEL-LJU team has taken the second place, scoring better than the previous state-of-the-art.\n",
      "========================================================\n",
      "The upsurge of prolific blogging and microblogging platforms enabled the abusers to spread negativity and threats greater than ever. Detecting the toxic portions substantially aids to moderate or exclude the abusive parts for maintaining sound online platforms. This paper describes our participation in the SemEval 2021 toxic span detection task. The task requires detecting spans that convey toxic remarks from the given text. We explore an ensemble of sequence labeling models including the BiLSTM-CRF, spaCy NER model with custom toxic tags, and fine-tuned BERT model to identify the toxic spans. Finally, a majority voting ensemble method is used to determine the unified toxic spans. Experimental results depict the competitive performance of our model among the participants.\n",
      "========================================================\n",
      "Sentiment analysis has come a long way for high-resource languages due to the availability of large annotated corpora. However, it still suffers from lack of training data for low-resource languages. To tackle this problem, we propose Conditional Language Adversarial Network (CLAN), an end-to-end neural architecture for cross-lingual sentiment analysis without cross-lingual supervision. CLAN differs from prior work in that it allows the adversarial training to be conditioned on both learned features and the sentiment prediction, to increase discriminativity for learned representation in the cross-lingual setting. Experimental results demonstrate that CLAN outperforms previous methods on the multilingual multi-domain Amazon review dataset. Our source code is released at https://github.com/hemanthkandula/clan.\n"
     ]
    }
   ],
   "source": [
    "print(abstract_list[s01['candidates'][dfi['indices'][442152][0]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][442152][0]][1]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][442152][1]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][442152][1]][1]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s01['candidates'][dfi['indices'][442152][2]][0]])\n",
    "#print('========================================================')\n",
    "#print(abstract_list[s01['candidates'][dfi['indices'][442152][2]][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "k=5 nearest neighbors and for s=0.1, yields clusters of 10 abstracts i.e. 5 candidate pairs each\n",
    "\n",
    "- This cluster is about Language tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for similarity threshhold, s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>candidates</th>\n",
       "      <th>jacc_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>0.749412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>0.772973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.708238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     s candidates  jacc_distance\n",
       "0  0.2     (0, 1)       0.749412\n",
       "1  0.2     (0, 3)       0.772973\n",
       "2  0.2     (0, 4)       0.708238"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s02.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jacc_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.749412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.772973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.708238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jacc_distance\n",
       "0       0.749412\n",
       "1       0.772973\n",
       "2       0.708238"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data2 = s02[['jacc_distance']]\n",
    "input_data2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[274592]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[68479]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    indices\n",
       "0  [274592]\n",
       "1   [68479]\n",
       "2       [2]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Nearest Neighbor, k = 1\n",
    "k = 1\n",
    "\n",
    "# fitting the model\n",
    "knn_model = NearestNeighbors(n_neighbors = k, algorithm = 'auto').fit(input_data2)\n",
    "distances, indices = knn_model.kneighbors(input_data2)\n",
    "\n",
    "# looping over the indices to make a list of the clusters\n",
    "ind = []\n",
    "for item in indices:\n",
    "    ind.append(item)\n",
    "\n",
    "# making a dataframe of it\n",
    "di = {'indices': ind}\n",
    "dfi = pd.DataFrame(di)    \n",
    "dfi.head(3) #this has two pairs of candidates in a cluster    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this paper, we develop Sindhi subjective lexicon using a merger of existing English resources: NRC lexicon, list of opinion words, SentiWordNet, Sindhi-English bilingual dictionary, and collection of Sindhi modifiers. The positive or negative sentiment score is assigned to each Sindhi opinion word. Afterwards, we determine the coverage of the proposed lexicon with subjectivity analysis. Moreover, we crawl multi-domain tweet corpus of news, sports, and finance. The crawled corpus is annotated by experienced annotators using the Doccano text annotation tool. The sentiment annotated corpus is evaluated by employing support vector machine (SVM), recurrent neural network (RNN) variants, and convolutional neural network (CNN).\n",
      "========================================================\n",
      "Sentiment analysis has come a long way for high-resource languages due to the availability of large annotated corpora. However, it still suffers from lack of training data for low-resource languages. To tackle this problem, we propose Conditional Language Adversarial Network (CLAN), an end-to-end neural architecture for cross-lingual sentiment analysis without cross-lingual supervision. CLAN differs from prior work in that it allows the adversarial training to be conditioned on both learned features and the sentiment prediction, to increase discriminativity for learned representation in the cross-lingual setting. Experimental results demonstrate that CLAN outperforms previous methods on the multilingual multi-domain Amazon review dataset. Our source code is released at https://github.com/hemanthkandula/clan.\n",
      "##############################################################################\n",
      "Natural Language Processing offers new insights into language data across almost all disciplines and domains, and allows us to corroborate and/or challenge existing knowledge. The primary hurdles to widening participation in and use of these new research tools are, first, a lack of coding skills in students across K-16, and in the population at large, and second, a lack of knowledge of how NLP-methods can be used to answer questions of disciplinary interest outside of linguistics and/or computer science. To broaden participation in NLP and improve NLP-literacy, we introduced a new tool web-based tool called Natural Language Processing 4 All (NLP4All). The intended purpose of NLP4All is to help teachers facilitate learning with and about NLP, by providing easy-to-use interfaces to NLP-methods, data, and analyses, making it possible for non- and novice-programmers to learn NLP concepts interactively.\n",
      "========================================================\n",
      "Neural models excel at extracting statistical patterns from large amounts of data, but struggle to learn patterns or reason about language from only a few examples. In this paper, we ask: Can we learn explicit rules that generalize well from only a few examples? We explore this question using program synthesis. We develop a synthesis model to learn phonology rules as programs in a domain-specific language. We test the ability of our models to generalize from few training examples using our new dataset of problems from the Linguistics Olympiad, a challenging set of tasks that require strong linguistic reasoning ability. In addition to being highly sample-efficient, our approach generates human-readable programs, and allows control over the generalizability of the learnt programs.\n"
     ]
    }
   ],
   "source": [
    "###  The equal sign separate abstracts in the same cluster but the hash-tagg sign separates clusters\n",
    "\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][1589][0]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][1589][0]][1]])\n",
    "print('##############################################################################')\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][3579][0]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][3579][0]][1]])\n",
    "#print('##############################################################################')\n",
    "#print(abstract_list[s02['candidates'][dfi['indices'][345][0]][0]])\n",
    "#print('========================================================')\n",
    "#print(abstract_list[s02['candidates'][dfi['indices'][345][0]][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "k=1 nearest neighbors and for  s=0.2, yields majorly no cluster, just the candidate pairs as shown above\n",
    "- The first pair is about human machine collaboration \n",
    "- The second pair is about sentence similarity, parsing or processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[247295, 12897, 274592]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[108945, 68479, 127469]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2, 207107, 166264]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   indices\n",
       "0  [247295, 12897, 274592]\n",
       "1  [108945, 68479, 127469]\n",
       "2      [2, 207107, 166264]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Nearest Neighbor, k = 3\n",
    "k = 3\n",
    "knn_model = NearestNeighbors(n_neighbors = k, algorithm = 'auto').fit(input_data2)\n",
    "distances, indices = knn_model.kneighbors(input_data2)\n",
    "\n",
    "# looping over the indices to make a list of the clusters\n",
    "ind = []\n",
    "for item in indices:\n",
    "    ind.append(item)\n",
    "\n",
    "di = {'indices': ind}\n",
    "dfi = pd.DataFrame(di)    \n",
    "dfi.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We introduce BERTweetFR, the first large-scale pre-trained language model for French tweets. Our model is initialised using a general-domain French language model CamemBERT which follows the base architecture of BERT. Experiments show that BERTweetFR outperforms all previous general-domain French language models on two downstream Twitter NLP tasks of offensiveness identification and named entity recognition. The dataset used in the offensiveness detection task is first created and annotated by our team, filling in the gap of such analytic datasets in French. We make our model publicly available in the transformers library with the aim of promoting future research in analytic tasks for French tweets.\n",
      "========================================================\n",
      "With the ever-increasing pace of research and high volume of scholarly communication, scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search, summarization, and analysis of scholarly documents. However, the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community, pool distributed efforts in this area, and enable shared access to published research, we held the 2nd Workshop on Scholarly Document Processing (SDP) at NAACL 2021 as a virtual event (https://sdproc.org/2021/). The SDP workshop consisted of a research track, three invited talks, and three Shared Tasks (LongSumm 2021, SCIVER, and 3C). The program was geared towards the application of NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges.\n",
      "========================================================\n",
      "We present the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021 (van der Goot et al., 2021a), which evaluates lexical-normalization systems on 12 social media datasets in 11 languages. We base our solution on a pre-trained byte-level language model, ByT5 (Xue et al., 2021a), which we further pre-train on synthetic data and then fine-tune on authentic normalization data. Our system achieves the best performance by a wide margin in intrinsic evaluation, and also the best performance in extrinsic evaluation through dependency parsing. The source code is released at https://github.com/ufal/multilexnorm2021 and the fine-tuned models at https://huggingface.co/ufal.\n",
      "========================================================\n",
      "Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the accuracy they offer for a variety of applications. Due to the significant environmental impact of deep learning, cost-benefit analysis including carbon footprint as well as accuracy measures has been suggested to better document the use of NLP methods for research or deployment. In this paper, we review the tools that are available to measure energy use and CO2 emissions of NLP methods. We describe the scope of the measures provided and compare the use of six tools (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) on named entity recognition experiments performed on different computational set-ups (local server vs. computing facility). Based on these findings, we propose actionable recommendations to accurately measure the environmental impact of NLP experiments.\n",
      "========================================================\n",
      "This paper describes our submissions for the Social Media Mining for Health (SMM4H) 2021 shared tasks. We participated in 2 tasks: (1) Classification, extraction and normalization of adverse drug effect (ADE) mentions in English tweets (Task-1) and (2) Classification of COVID-19 tweets containing symptoms (Task-6). Our approach for the first task uses the language representation model RoBERTa with a binary classification head. For the second task, we use BERTweet, based on RoBERTa. Fine-tuning is performed on the pre-trained models for both tasks. The models are placed on top of a custom domain-specific pre-processing pipeline. Our system ranked first among all the submissions for subtask-1(a) with an F1-score of 61{\\%}. For subtask-1(b), our system obtained an F1-score of 50{\\%} with improvements up to +8{\\%} F1 over the median score across all submissions. The BERTweet model achieved an F1 score of 94{\\%} on SMM4H 2021 Task-6.\n"
     ]
    }
   ],
   "source": [
    "print(abstract_list[s02['candidates'][dfi['indices'][41264][0]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][41264][0]][1]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][41264][1]][0]])\n",
    "#print('========================================================')\n",
    "#print(abstract_list[s02['candidates'][dfi['indices'][412652][1]][1]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][41264][2]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][41264][2]][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "k=3 nearest neighbor and for s=0.2, yields clusters of 6 abstracts i.e. 3 candidate pairs each\n",
    "\n",
    "- This cluster is about various classification systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[274592, 0, 247295, 12897, 20722]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[127469, 6483, 153000, 68479, 108945]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2, 207107, 166264, 193469, 30877]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 indices\n",
       "0      [274592, 0, 247295, 12897, 20722]\n",
       "1  [127469, 6483, 153000, 68479, 108945]\n",
       "2     [2, 207107, 166264, 193469, 30877]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Nearest Neighbor, k = 5\n",
    "k = 5\n",
    "knn_model = NearestNeighbors(n_neighbors = k, algorithm = 'auto').fit(input_data2)\n",
    "distances, indices = knn_model.kneighbors(input_data2)\n",
    "\n",
    "# looping over the indices to make a list of the clusters\n",
    "ind = []\n",
    "for item in indices:\n",
    "    ind.append(item)\n",
    "\n",
    "di = {'indices': ind}\n",
    "dfi = pd.DataFrame(di)    \n",
    "dfi.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the accuracy they offer for a variety of applications. Due to the significant environmental impact of deep learning, cost-benefit analysis including carbon footprint as well as accuracy measures has been suggested to better document the use of NLP methods for research or deployment. In this paper, we review the tools that are available to measure energy use and CO2 emissions of NLP methods. We describe the scope of the measures provided and compare the use of six tools (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) on named entity recognition experiments performed on different computational set-ups (local server vs. computing facility). Based on these findings, we propose actionable recommendations to accurately measure the environmental impact of NLP experiments.\n",
      "========================================================\n",
      "This paper describes our submissions for the Social Media Mining for Health (SMM4H) 2021 shared tasks. We participated in 2 tasks: (1) Classification, extraction and normalization of adverse drug effect (ADE) mentions in English tweets (Task-1) and (2) Classification of COVID-19 tweets containing symptoms (Task-6). Our approach for the first task uses the language representation model RoBERTa with a binary classification head. For the second task, we use BERTweet, based on RoBERTa. Fine-tuning is performed on the pre-trained models for both tasks. The models are placed on top of a custom domain-specific pre-processing pipeline. Our system ranked first among all the submissions for subtask-1(a) with an F1-score of 61{\\%}. For subtask-1(b), our system obtained an F1-score of 50{\\%} with improvements up to +8{\\%} F1 over the median score across all submissions. The BERTweet model achieved an F1 score of 94{\\%} on SMM4H 2021 Task-6.\n",
      "========================================================\n",
      "We introduce BERTweetFR, the first large-scale pre-trained language model for French tweets. Our model is initialised using a general-domain French language model CamemBERT which follows the base architecture of BERT. Experiments show that BERTweetFR outperforms all previous general-domain French language models on two downstream Twitter NLP tasks of offensiveness identification and named entity recognition. The dataset used in the offensiveness detection task is first created and annotated by our team, filling in the gap of such analytic datasets in French. We make our model publicly available in the transformers library with the aim of promoting future research in analytic tasks for French tweets.\n",
      "========================================================\n",
      "We present the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021 (van der Goot et al., 2021a), which evaluates lexical-normalization systems on 12 social media datasets in 11 languages. We base our solution on a pre-trained byte-level language model, ByT5 (Xue et al., 2021a), which we further pre-train on synthetic data and then fine-tune on authentic normalization data. Our system achieves the best performance by a wide margin in intrinsic evaluation, and also the best performance in extrinsic evaluation through dependency parsing. The source code is released at https://github.com/ufal/multilexnorm2021 and the fine-tuned models at https://huggingface.co/ufal.\n",
      "========================================================\n",
      "In recent years, speech synthesis system can generate speech with high speech quality. However, multi-speaker text-to-speech (TTS) system still require large amount of speech data for each target speaker. In this study, we would like to construct a multi-speaker TTS system by incorporating two sub modules into artificial neural network-based speech synthesis system to alleviate this problem. First module is to add speaker embedding into encoding module for generating speech while a large amount of the speech data from target speaker is not necessary. For speaker embedding method, in our study, two main speaker embedding methods, namely speaker verification embedding and voice conversion embedding, are compared to deciding which one is suitable for our personalized TTS system. Second, we substituted the conventional post-net module, which is adopted to enhance the output spectrum sequence, to further improving the speech quality of the generated speech utterance. Here, a post-filter network is used. Finally, experiment results showed that the speaker embedding is useful by adding it into encoding module and the resultant speech utterance indeed perceived as the target speaker. Also, the post-filter network not only improving the speech quality and also enhancing the speaker similarity of the generated speech utterances. The constructed TTS system can generate a speech utterance of the target speaker in fewer than 2 seconds. In the future, we would like to further investigate the controllability of the speaking rate or perceived emotion state of the generated speech.\n"
     ]
    }
   ],
   "source": [
    "print(abstract_list[s02['candidates'][dfi['indices'][41264][0]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][41264][0]][1]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][41264][1]][0]])\n",
    "#print('========================================================')\n",
    "#print(abstract_list[s02['candidates'][dfi['indices'][412652][1]][1]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][41264][2]][0]])\n",
    "print('========================================================')\n",
    "print(abstract_list[s02['candidates'][dfi['indices'][41264][2]][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "\n",
    "k=5 nearest neighbors and for s=0.2, yields clusters of 10 abstracts each (i.e. 5 candidate pairs each)\n",
    "\n",
    "- This cluster is about human dialogue catalog processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
