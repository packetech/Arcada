{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This project will explore and analyze the information stored in a particular dataset. In this case, the ACL Anthology dataset (https://aclanthology.org/). We will explore different techniques for obtaining valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Mining information from Text Data\n",
    "Using the whole anthologies abstract dataset. Extract a list of the authors and editors per publication and create baskets and perform a search of similar items, for example:\n",
    "\n",
    "- basket 1: Mostafazadeh Davani Aida,Kiela Douwe,Lambert Mathias,Vidgen, Bertie Prabhakaran Vinodkumar, Waseem, Zeerak\n",
    "- basket 2: Singh Sumer, Li Sheng\n",
    "\n",
    "1. Find the frequent pair of items (2-tuples) using the naïve, A-priori and PCY algorithms. For each of these compare the time of execution and results for supports s=10, 50, 100. Comment your results.\n",
    "\n",
    "2. For the PCY algorithm, create up to 5 compact hash tables. What is the difference in results and time of execution for 1,2,3,4 and 5 tables? Comment your results.\n",
    "\n",
    "3. Find the final list of k-frequent items (k-tuples) for k=3 and 4. Experiment a bit and describe the best value for the support in each case. Warning: You can use any of the three algorithms, but be careful, because the algorithm can take too long if you don't chose it properly (well, basically don't use the naïve approach ;)).\n",
    "\n",
    "4. Using one of the results of the previous items, for one k (k=2 or 3) find the possible clusters using the 1-NN criteria. Comment your results.\n",
    "\n",
    "1-NN means that if you have a tuple {A,B,C} and {C,E,F} then because they share one element {C}, then they belong to the same cluster {A,B,C,E,F}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "#from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import binascii\n",
    "from time import time\n",
    "\n",
    "from urllib import request\n",
    "import gzip\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract dataset\n",
    "\n",
    "url1 = \"https://aclanthology.org/anthology+abstracts.bib.gz\"\n",
    "file_name1 = re.split(pattern='/', string=url1)[-1]\n",
    "r1 = request.urlretrieve(url=url1, filename=file_name1)\n",
    "txt1 = re.split(pattern=r'\\.', string=file_name1)[0] + \".txt\"\n",
    "\n",
    "# Extract it\n",
    "with gzip.open(file_name1, 'rb') as f_in:\n",
    "    with open(txt1, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "fname = txt1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various Functions\n",
    "#==================#\n",
    "\n",
    "# abstract extracting function\n",
    "# fname is the file name of the document containing all abstract\n",
    "# n is the number of abstracts that we will extract\n",
    "def read_abstracts(fname,n):\n",
    "    abs = [] #initialize a list variable\n",
    "    with open(fname, 'r', encoding=\"utf-8\") as f:\n",
    "        i = 0\n",
    "        # skip all lines until abstract\n",
    "        for line in f:\n",
    "            if \"abstract =\" in line:\n",
    "                pattern = '\"'\n",
    "                abstract = re.split(pattern ,line, flags=re.IGNORECASE)[1].split('\"')[0]\n",
    "                if len(abstract)<5: # takes care of empty abstracts\n",
    "                    pass\n",
    "                    \n",
    "                else:\n",
    "                    abs.append(abstract) # append each abstract to the list\n",
    "                    i = i + 1\n",
    "                if i == n:  # number of abstracts to extract\n",
    "                    return abs\n",
    "        \n",
    "        return abs\n",
    "#=================================================\n",
    "    \n",
    "# Shingle function\n",
    "# k is the number of shingles\n",
    "\n",
    "def get_shingles(abstract, k):\n",
    "    \"\"\"Get all shingles from requested file (hashes of these shingles)\n",
    "    \"\"\"\n",
    "    L = len(abstract)\n",
    "    shingles = set()  # we use a set to automatically eliminate duplicates\n",
    "    for i in range(L-k+1):\n",
    "        shingle = abstract[i:i+k]\n",
    "        crc = binascii.crc32(shingle.encode('utf-8')) # hash the shingle to a 32-bit integer\n",
    "        shingles.add(crc)\n",
    "    return shingles\n",
    "#=================================================\n",
    "\n",
    "# jaccard similarity score Function\n",
    "def jaccard_similarity_score(x, y, errors='ignore'):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity J (A,B) = | Intersection (A,B) | /\n",
    "                                    | Union (A,B) |\n",
    "    \"\"\"\n",
    "    intersection_cardinality = len(set(x).intersection(set(y)))\n",
    "    union_cardinality = len(set(x).union(set(y)))\n",
    "    if float(union_cardinality) == 0:\n",
    "        ja = 0\n",
    "    else:\n",
    "        ja = intersection_cardinality / float(union_cardinality)\n",
    "    return ja\n",
    "#=================================================\n",
    "\n",
    "# similarity functions\n",
    "# k is number of shingles and s is the similarity thresholds \n",
    "# abstract_list is the list of 1000 abstracts\n",
    "\n",
    "def similar_items(abstract_list, k, s):\n",
    "    candidates = []\n",
    "    #abstract_list = read_abstracts(fname,n)\n",
    "    for pair in itertools.combinations(abstract_list,2):\n",
    "        js = jaccard_similarity_score(get_shingles(pair[0], k),get_shingles(pair[1], k))\n",
    "        \n",
    "        if js > s:\n",
    "            #print(pair)\n",
    "            candidates.append(pair)\n",
    "            \n",
    "    return candidates\n",
    "#=================================================\n",
    "\n",
    "\n",
    "# fast implementation of Minhashing algorithm\n",
    "# computes all random hash functions for a shingle at once, using vector operations\n",
    "# also finds element-wise minimum of two vectors efficiently\n",
    "def minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig):\n",
    "    signature = numpy.ones((nsig,)) * (maxShingleID + 1)\n",
    "\n",
    "    for ShingleID in shingles:\n",
    "        hashCodes = ((A*ShingleID + B) % nextPrime) % maxShingleID\n",
    "        numpy.minimum(signature, hashCodes, out=signature)\n",
    "\n",
    "    return signature\n",
    "#=================================================\n",
    "\n",
    "# candidate pair function\n",
    "def candidate_pair(abstract_list, k, s):\n",
    "    signatures = []  # signatures for all files\n",
    "    for abstract in abstract_list:\n",
    "        shingles = get_shingles(abstract, k)\n",
    "        signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "        signatures.append(signature)\n",
    "        \n",
    "    Nfiles = len(signatures)\n",
    "    #startTime = time.time()\n",
    "    candidates = []\n",
    "    for i in range(Nfiles):\n",
    "        for j in range(i+1, Nfiles):\n",
    "            Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "            if Jsim >= s:                                      # two vectors, equivalente to Jaccard\n",
    "                candidates.append((i,j))\n",
    "                \n",
    "            \n",
    "    return len(candidates)\n",
    "#=================================================\n",
    "\n",
    "# Moditied Function for jaccard similarity\n",
    "\n",
    "def jaccard_similarity_score_mod2(a, b, shingles_list, errors='ignore'): \n",
    "    \n",
    "    sha = shingles_list[a]\n",
    "    shingles_vector_a = sha\n",
    " \n",
    "    shb = shingles_list[b]\n",
    "    shingles_vector_b = shb\n",
    "\n",
    "    jsc = jaccard_similarity_score(shingles_vector_a, shingles_vector_b)\n",
    "    \n",
    "    return jsc\n",
    "#=================================================\n",
    "\n",
    "# LSH candidates function\n",
    "def LSH(signatures, bands, rows, Ab, Bb, nextPrime, maxShingleID, s, shingles_list):\n",
    "    \"\"\"Locality Sensitive Hashing\n",
    "    \"\"\"\n",
    "    numItems = signatures.shape[1]\n",
    "    signBands = numpy.array_split(signatures, bands, axis=0) \n",
    "    candidates = set()\n",
    "    for nb in range(bands):\n",
    "        hashTable = {}\n",
    "        for ni in range(numItems):\n",
    "            item = signBands[nb][:,ni]\n",
    "            hash = (numpy.dot(Ab[nb,:], item) + Bb[nb]) % nextPrime % maxShingleID\n",
    "            if hash not in hashTable:\n",
    "                hashTable[hash] = [ni]\n",
    "            else:\n",
    "                hashTable[hash].append(ni)\n",
    "        for _,items in hashTable.items():\n",
    "            if len(items) > 1:\n",
    "                L = len(items)\n",
    "                for i in range(L-1):\n",
    "                    for j in range(i+1, L):\n",
    "                        cand = [items[i], items[j]]\n",
    "                        a = items[i]\n",
    "                        b = items[j]\n",
    "                        jsim = jaccard_similarity_score_mod2(a,b, shingles_list) #jaccard similarity function call\n",
    "                        if jsim >= s:\n",
    "                            numpy.sort(cand)\n",
    "                            candidates.add(tuple(cand))\n",
    "    return candidates\n",
    "#=================================================\n",
    "\n",
    "# LSH candidates length function\n",
    "def LSH_candidates(abstract_list, k, s):\n",
    "    signatures = []  # signatures for all files\n",
    "    shingles_list =[]\n",
    "    for abstract in abstract_list:\n",
    "        shingles = get_shingles(abstract, k)\n",
    "        signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "        signatures.append(signature)\n",
    "        shingles_list.append(shingles)\n",
    "        \n",
    "    \n",
    "    A2 = numpy.random.randint(0, nextPrime/2, size=(bands, rows),dtype=numpy.int64)  # now we need a vector of A parameters for each band\n",
    "    B2 = numpy.random.randint(0, nextPrime/2, size=(bands, ),dtype=numpy.int64)\n",
    "    signatures = numpy.array(signatures).T  # LSH needs a matrix of signatures, not a list of vectors\n",
    "\n",
    "  \n",
    "    candidates = LSH(signatures, bands, rows, A2, B2, nextPrime, maxShingleID, s, shingles_list)\n",
    "   \n",
    "    \n",
    "    return len(candidates)\n",
    "#=================================================\n",
    "\n",
    "# New minHash function for Task1 number2 a\n",
    "def Sim_Method_Property(abstract_list,k,s,bands,rows):\n",
    "    \n",
    "    nsig = bands*rows  # hashing function: number of elements in signature, or the number of different random hash functions\n",
    "\n",
    "    #maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "    #nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "\n",
    "    #A = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "    #B = numpy.random.randint(0, nextPrime, size=(nsig,),dtype=numpy.int64)\n",
    "    \n",
    "    startTime = time.time()\n",
    "    cand = candidate_pair(abstract_list, k, s)\n",
    "    execTime = round(((time.time() - startTime)),2)\n",
    "\n",
    "    minHash_k.append(k)\n",
    "    minHash_s.append(s)\n",
    "    Hashing_fn.append(nsig)\n",
    "    minHash_sim.append(cand)\n",
    "    minHash_execT.append(execTime)\n",
    "    \n",
    "    dict = {'k': minHash_k, 's': minHash_s, 'Hashing_fn': Hashing_fn,'#sim': minHash_sim, 'execTime(sec)': minHash_execT} \n",
    "    df = pd.DataFrame(dict)\n",
    "    \n",
    "    return df\n",
    "#=================================================\n",
    "\n",
    "# New LSH function for Task1 number2 b\n",
    "def Sim_Method_Property2(abstract_list,k,s,bands,rows):\n",
    "    \n",
    "    \n",
    "    nsig = bands*rows  # hashing function: number of elements in signature, or the number of different random hash functions\n",
    "    \n",
    "    startTime = time.time()\n",
    "    candi = LSH_candidates(abstract_list,k, s)\n",
    "    execTime = round(((time.time() - startTime)),2)\n",
    "    \n",
    "    LSH_k.append(k)\n",
    "    LSH_s.append(s)\n",
    "    Hashing_fn2.append(nsig)\n",
    "    LSH_sim.append(candi)\n",
    "    LSH_execT.append(execTime)\n",
    "    \n",
    "    dict = {'k': LSH_k, 's': LSH_s, 'Hashing_fn': Hashing_fn2,'#sim': LSH_sim, 'execTime(sec)': LSH_execT} \n",
    "    df = pd.DataFrame(dict)\n",
    "    \n",
    "    return df\n",
    "#=================================================\n",
    "\n",
    "# functions for Task1 number 3\n",
    "# Jaccard distance calculator function\n",
    "\n",
    "def jacc_dist_calc(abstract_list,k,s,bands,rows):\n",
    "    \n",
    "    nsig = bands*rows  # hashing function: number of elements in signature, or the number of different random hash functions\n",
    "    \n",
    "    jd_df = candidate_pair_jacc_dist(abstract_list, k, s, nsig)\n",
    "\n",
    "    return jd_df\n",
    "#==============\n",
    "\n",
    "# A modified candidate pair function\n",
    "\n",
    "def candidate_pair_jacc_dist(abstract_list, k, s, nsig):\n",
    "    signatures = []  # signatures for all files\n",
    "    shingles_list = []\n",
    "    for abstract in abstract_list:\n",
    "        shingles = get_shingles(abstract, k)\n",
    "        signature = minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "        signatures.append(signature)\n",
    "        shingles_list.append(shingles)\n",
    "        \n",
    "    Nfiles = len(signatures)\n",
    "    candidates = []\n",
    "    jaccard_distance = []\n",
    "    s_list = []\n",
    "    #k_list = []\n",
    "    #h_fn_list = []\n",
    "    #sign1 = []\n",
    "    #sign2 = []\n",
    "    for i in range(Nfiles):\n",
    "        for j in range(i+1, Nfiles):\n",
    "            Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "            if Jsim >= s:                                      # two vectors, equivalente to Jaccard\n",
    "                #a = i\n",
    "                #b = j\n",
    "                js = jaccard_similarity_score_mod2(i,j, shingles_list)\n",
    "                jaccard_distance.append(1-js) # jaccard distance calculations\n",
    "                s_list.append(s)\n",
    "                #k_list.append(k)\n",
    "                #h_fn_list.append(nsig)\n",
    "                candidates.append((i,j))\n",
    "                #sign1.append(signatures[i])\n",
    "                #sign2.append(signatures[j])\n",
    "    \n",
    "    dict = {'s': s_list, 'candidates': candidates, 'jacc_distance': jaccard_distance} \n",
    "    df = pd.DataFrame(dict)\n",
    "    return df    #len(candidates)\n",
    "#=================================================\n",
    "#=================================================\n",
    "#=================================================\n",
    "# naive algorithm function for Task2\n",
    "\n",
    "def naive_items(s):\n",
    "    start = time.time() # start time\n",
    "    \n",
    "    def get_C(k):\n",
    "        C = {}\n",
    "        for key in readdata_hands_on(k):  # False report\n",
    "            if key not in C:\n",
    "                C[key] = 1\n",
    "            else:\n",
    "                C[key] += 1\n",
    "        #print(\"Took {}s for k={}\".format((time.time() - start), k))\n",
    "        return C\n",
    "\n",
    "\n",
    "    C1 = get_C(1)\n",
    "    C2 = get_C(2)\n",
    "    \n",
    "    \n",
    "     \n",
    "    L2 = {}\n",
    "    for key, n in C2.items():\n",
    "        if n >= s:\n",
    "            L2[key] = n\n",
    "    \n",
    "    # append results\n",
    "    s_list.append(s)\n",
    "    L2_counts_list.append(len(L2))\n",
    "    Wall_time_list_ns.append(time.time() - start)\n",
    "    return\n",
    "#=================================================\n",
    "\n",
    "# a-priori algorithm function for Task2\n",
    "\n",
    "def apriori_items(s):\n",
    "    \n",
    "    start = time.time() # start time\n",
    "    N = s\n",
    "    # find frequent individual items\n",
    "    C1 = {}\n",
    "    for key in readdata_hands_on(k=1):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "        \n",
    "    #print(\"{} items\".format(len(C1)))\n",
    "    \n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= N:\n",
    "            L1[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L1), N))\n",
    "    \n",
    "    # List comprehensions in python\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()]) \n",
    "    #print(len(C2_items))\n",
    "    \n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata_hands_on(k=2):\n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "        \n",
    "    #print(\"{} items\".format(len(C2)))\n",
    "    \n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= N:\n",
    "            L2[key] = count\n",
    "    #print('A-priori: {} items with >{} occurances'.format(len(L2), N))\n",
    "    \n",
    "    # append results\n",
    "    s_list.append(s)\n",
    "    L2_counts_list.append(len(L2))\n",
    "    Wall_time_list_ns.append(time.time() - start)\n",
    "    return\n",
    "#=================================================\n",
    "\n",
    "def readdata_hands_on(k, fname='anthology_authors.csv'):\n",
    "    \n",
    "    with open(fname, \"rt\", encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            C_k  = line.rstrip().split(';')\n",
    "            for itemset in itertools.combinations(C_k, k):\n",
    "                    yield frozenset(itemset)\n",
    "#=================================================\n",
    "\n",
    "# pcy algorithm function\n",
    "\n",
    "def pcy_items(s):\n",
    "    \n",
    "    start = time.time() # start time\n",
    "    N = s # frequency threshold\n",
    "    \n",
    "    # hash table\n",
    "    max_hash1 = 10 * 1000000\n",
    "    H1 = np.zeros((max_hash1, ), dtype=np.int)\n",
    "\n",
    "    for key in readdata_hands_on(k=2):\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        H1[hash_cell_1] += 1\n",
    "        \n",
    "    # find frequent individual items\n",
    "    C1 = {}\n",
    "    for key in readdata_hands_on(k=1):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "        \n",
    "    #print(\"{} items\".format(len(C1)))\n",
    "    \n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= N:\n",
    "            L1[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L1), N))\n",
    "    \n",
    "    # List comprehensions in python\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()])\n",
    "    \n",
    "    #print(len(C2_items))\n",
    "    \n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata_hands_on(k=2):\n",
    "        # hash-based filtering stage from PCY\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        if H1[hash_cell_1] < N:\n",
    "            continue\n",
    "\n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "        \n",
    "    #print(\"{} items\".format(len(C2)))\n",
    "    \n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= N:\n",
    "            L2[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L2), N))\n",
    "    \n",
    "    # append results\n",
    "    s_list.append(s)\n",
    "    L2_counts_list.append(len(L2))\n",
    "    Wall_time_list_ns.append(time.time() - start)\n",
    "    return L2\n",
    "#=================================================\n",
    "\n",
    "def readdata_hands_on2(k,report=False,fname='anthology_authors.csv'):\n",
    "    \n",
    "    with open(fname, \"rt\", encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            C_k  = line.rstrip().split(';')\n",
    "            for itemset in itertools.combinations(C_k, k):\n",
    "                    yield frozenset(itemset) \n",
    "#=================================================\n",
    "                    \n",
    "# Task 2 hash tables functions\n",
    "\n",
    "##### PCY Algorithm\n",
    "\n",
    "# One compact hash table \n",
    "# PCY algorithm function\n",
    "\n",
    "def oneHashTablePCY(s):\n",
    "    \n",
    "    start = time.time() # start time\n",
    "    N = s # frequency threshold\n",
    "    \n",
    "    # hash table\n",
    "    max_hash1 = 10 * 1000000\n",
    "    H1 = np.zeros((max_hash1, ), dtype=np.int)\n",
    "\n",
    "    for key in readdata_hands_on2(k=2, report=False):\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        H1[hash_cell_1] += 1\n",
    "        \n",
    "        \n",
    "    #### === one compact hash table ====####\n",
    "    H_good_1 = set(np.where(H1 >= N)[0])\n",
    "    del H1\n",
    "    \n",
    "    # find frequent individual items\n",
    "    C1 = {}\n",
    "    for key in readdata_hands_on2(k=1):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "        \n",
    "    #print(\"{} items\".format(len(C1)))\n",
    "    \n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= N:\n",
    "            L1[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L1), N))\n",
    "    \n",
    "    # List comprehensions in python\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()])\n",
    "\n",
    "    \n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata_hands_on2(k=2):\n",
    "        # hash-based filtering stage from PCY\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        if hash_cell_1 not in H_good_1:\n",
    "            continue\n",
    "\n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "        \n",
    "    #print(\"{} items\".format(len(C2)))\n",
    "\n",
    "   \n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= N:\n",
    "            L2[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L2), N))\n",
    "    \n",
    "    # append results\n",
    "    s_list.append(s)\n",
    "    L2_counts_list.append(len(L2))\n",
    "    Wall_time_list_ns.append(time.time() - start)\n",
    "    return\n",
    "#==============\n",
    "\n",
    "# Two compact hash table \n",
    "# PCY algorithm function\n",
    "\n",
    "def twoHashTablePCY(s):\n",
    "    \n",
    "    start = time.time() # start time\n",
    "    N = s # frequency threshold\n",
    "    \n",
    "    # hash tables\n",
    "    max_hash1 = 5*1000000-673\n",
    "    max_hash2 = 5*1000000+673\n",
    "    \n",
    "    H1 = np.zeros((max_hash1,), dtype=np.int)\n",
    "    H2 = np.zeros((max_hash2,), dtype=np.int)\n",
    "\n",
    "    for key in readdata_hands_on2(k=2, report=False):\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        H1[hash_cell_1] += 1\n",
    "        hash_cell_2 = hash(key) % max_hash2\n",
    "        H2[hash_cell_2] += 1\n",
    "        \n",
    "    #### === two compact hash tables ====####\n",
    "    H_good_1 = set(np.where(H1 >= N)[0])\n",
    "    H_good_2 = set(np.where(H2 >= N)[0])\n",
    "    del H1\n",
    "    del H2\n",
    "    \n",
    "    # find frequent individual items\n",
    "    C1 = {}\n",
    "    for key in readdata_hands_on2(k=1):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "        \n",
    "    #print(\"{} items\".format(len(C1)))\n",
    "    \n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= N:\n",
    "            L1[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L1), N))\n",
    "    \n",
    "    # List comprehensions in python\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()])\n",
    "\n",
    "    \n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata_hands_on2(k=2):\n",
    "        # hash-based filtering stage from PCY\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        if hash_cell_1 not in H_good_1:\n",
    "            continue\n",
    "        \n",
    "        hash_cell_2 = hash(key) % max_hash2\n",
    "        if hash_cell_2 not in H_good_2:\n",
    "            continue\n",
    "                \n",
    "\n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "        \n",
    "    #print(\"{} items\".format(len(C2)))\n",
    "\n",
    "   \n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= N:\n",
    "            L2[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L2), N))\n",
    "    \n",
    "    # append results\n",
    "    s_list.append(s)\n",
    "    L2_counts_list.append(len(L2))\n",
    "    Wall_time_list_ns.append(time.time() - start)\n",
    "    return\n",
    "#==============\n",
    "\n",
    "# Three compact hash table \n",
    "# PCY algorithm function\n",
    "\n",
    "def threeHashTablePCY(s):\n",
    "    \n",
    "    start = time.time() # start time\n",
    "    N = s # frequency threshold\n",
    "    \n",
    "    # hash tables\n",
    "    max_hash1 = 5*1000000-673\n",
    "    max_hash2 = 5*1000000+673\n",
    "    max_hash3 = 5*1000000-128\n",
    "    \n",
    "    H1 = np.zeros((max_hash1,), dtype=np.int)\n",
    "    H2 = np.zeros((max_hash2,), dtype=np.int)\n",
    "    H3 = np.zeros((max_hash3,), dtype=np.int)\n",
    "\n",
    "    for key in readdata_hands_on2(k=2, report=False):\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        H1[hash_cell_1] += 1\n",
    "        hash_cell_2 = hash(key) % max_hash2\n",
    "        H2[hash_cell_2] += 1\n",
    "        hash_cell_3 = hash(key) % max_hash3\n",
    "        H3[hash_cell_3] += 1\n",
    "        \n",
    "        \n",
    "    #### === three compact hash tables ====####\n",
    "    H_good_1 = set(np.where(H1 >= N)[0])\n",
    "    H_good_2 = set(np.where(H2 >= N)[0])\n",
    "    H_good_3 = set(np.where(H3 >= N)[0])\n",
    "    del H1\n",
    "    del H2\n",
    "    del H3\n",
    "    \n",
    "    # find frequent individual items\n",
    "    C1 = {}\n",
    "    for key in readdata_hands_on2(k=1):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "        \n",
    "    #print(\"{} items\".format(len(C1)))\n",
    "    \n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= N:\n",
    "            L1[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L1), N))\n",
    "    \n",
    "    # List comprehensions in python\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()])\n",
    "\n",
    "    \n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata_hands_on2(k=2):\n",
    "        # hash-based filtering stage from PCY\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        if hash_cell_1 not in H_good_1:\n",
    "            continue\n",
    "        \n",
    "        hash_cell_2 = hash(key) % max_hash2\n",
    "        if hash_cell_2 not in H_good_2:\n",
    "            continue\n",
    "         \n",
    "        hash_cell_3 = hash(key) % max_hash3\n",
    "        if hash_cell_3 not in H_good_3:\n",
    "            continue\n",
    "\n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "        \n",
    "    #print(\"{} items\".format(len(C2)))\n",
    "\n",
    "   \n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= N:\n",
    "            L2[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L2), N))\n",
    "    \n",
    "    # append results\n",
    "    s_list.append(s)\n",
    "    L2_counts_list.append(len(L2))\n",
    "    Wall_time_list_ns.append(time.time() - start)\n",
    "    return\n",
    "#==============\n",
    "\n",
    "# Four compact hash table \n",
    "# PCY algorithm function\n",
    "\n",
    "def fourHashTablePCY(s):\n",
    "    \n",
    "    start = time.time() # start time\n",
    "    N = s # frequency threshold\n",
    "    \n",
    "    # hash tables\n",
    "    max_hash1 = 5*1000000-673\n",
    "    max_hash2 = 5*1000000+673\n",
    "    max_hash3 = 5*1000000-128\n",
    "    max_hash4 = 5*1000000+128\n",
    "    \n",
    "    H1 = np.zeros((max_hash1,), dtype=np.int)\n",
    "    H2 = np.zeros((max_hash2,), dtype=np.int)\n",
    "    H3 = np.zeros((max_hash3,), dtype=np.int)\n",
    "    H4 = np.zeros((max_hash4,), dtype=np.int)\n",
    "\n",
    "    for key in readdata_hands_on2(k=2, report=False):\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        H1[hash_cell_1] += 1\n",
    "        hash_cell_2 = hash(key) % max_hash2\n",
    "        H2[hash_cell_2] += 1\n",
    "        hash_cell_3 = hash(key) % max_hash3\n",
    "        H3[hash_cell_3] += 1\n",
    "        hash_cell_4 = hash(key) % max_hash4\n",
    "        H4[hash_cell_4] += 1\n",
    "        \n",
    "    #### === four compact hash tables ====####\n",
    "    H_good_1 = set(np.where(H1 >= N)[0])\n",
    "    H_good_2 = set(np.where(H2 >= N)[0])\n",
    "    H_good_3 = set(np.where(H3 >= N)[0])\n",
    "    H_good_4 = set(np.where(H4 >= N)[0])\n",
    "    del H1\n",
    "    del H2\n",
    "    del H3\n",
    "    del H4\n",
    "    \n",
    "    # find frequent individual items\n",
    "    C1 = {}\n",
    "    for key in readdata_hands_on2(k=1):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "        \n",
    "    #print(\"{} items\".format(len(C1)))\n",
    "    \n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= N:\n",
    "            L1[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L1), N))\n",
    "    \n",
    "    # List comprehensions in python\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()])\n",
    "\n",
    "    \n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata_hands_on2(k=2):\n",
    "        # hash-based filtering stage from PCY\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        if hash_cell_1 not in H_good_1:\n",
    "            continue\n",
    "        \n",
    "        hash_cell_2 = hash(key) % max_hash2\n",
    "        if hash_cell_2 not in H_good_2:\n",
    "            continue\n",
    "         \n",
    "        hash_cell_3 = hash(key) % max_hash3\n",
    "        if hash_cell_3 not in H_good_3:\n",
    "            continue\n",
    "\n",
    "        hash_cell_4 = hash(key) % max_hash4\n",
    "        if hash_cell_4 not in H_good_4:\n",
    "            continue\n",
    "        \n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "        \n",
    "    #print(\"{} items\".format(len(C2)))\n",
    "\n",
    "   \n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= N:\n",
    "            L2[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L2), N))\n",
    "    \n",
    "    # append results\n",
    "    s_list.append(s)\n",
    "    L2_counts_list.append(len(L2))\n",
    "    Wall_time_list_ns.append(time.time() - start)\n",
    "    return\n",
    "#==============\n",
    "\n",
    "# Five compact hash table \n",
    "# PCY algorithm function\n",
    "\n",
    "def fiveHashTablePCY(s):\n",
    "    \n",
    "    start = time.time() # start time\n",
    "    N = s # frequency threshold\n",
    "    \n",
    "    # hash tables\n",
    "    max_hash1 = 5*1000000-673\n",
    "    max_hash2 = 5*1000000+673\n",
    "    max_hash3 = 5*1000000-128\n",
    "    max_hash4 = 5*1000000+128\n",
    "    max_hash5 = 5*1000000+256\n",
    "    \n",
    "    H1 = np.zeros((max_hash1,), dtype=np.int)\n",
    "    H2 = np.zeros((max_hash2,), dtype=np.int)\n",
    "    H3 = np.zeros((max_hash3,), dtype=np.int)\n",
    "    H4 = np.zeros((max_hash4,), dtype=np.int)\n",
    "    H5 = np.zeros((max_hash5,), dtype=np.int)\n",
    "\n",
    "    for key in readdata_hands_on2(k=2, report=False):\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        H1[hash_cell_1] += 1\n",
    "        hash_cell_2 = hash(key) % max_hash2\n",
    "        H2[hash_cell_2] += 1\n",
    "        hash_cell_3 = hash(key) % max_hash3\n",
    "        H3[hash_cell_3] += 1\n",
    "        hash_cell_4 = hash(key) % max_hash4\n",
    "        H4[hash_cell_4] += 1\n",
    "        hash_cell_5 = hash(key) % max_hash5\n",
    "        H5[hash_cell_5] += 1\n",
    "        \n",
    "    #### === five compact hash tables ====####\n",
    "    H_good_1 = set(np.where(H1 >= N)[0])\n",
    "    H_good_2 = set(np.where(H2 >= N)[0])\n",
    "    H_good_3 = set(np.where(H3 >= N)[0])\n",
    "    H_good_4 = set(np.where(H4 >= N)[0])\n",
    "    H_good_5 = set(np.where(H5 >= N)[0])\n",
    "    del H1\n",
    "    del H2\n",
    "    del H3\n",
    "    del H4\n",
    "    del H5\n",
    "    \n",
    "    # find frequent individual items\n",
    "    C1 = {}\n",
    "    for key in readdata_hands_on2(k=1):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "        \n",
    "    #print(\"{} items\".format(len(C1)))\n",
    "    \n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= N:\n",
    "            L1[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L1), N))\n",
    "    \n",
    "    # List comprehensions in python\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()])\n",
    "\n",
    "    \n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata_hands_on2(k=2):\n",
    "        # hash-based filtering stage from PCY\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        if hash_cell_1 not in H_good_1:\n",
    "            continue\n",
    "        \n",
    "        hash_cell_2 = hash(key) % max_hash2\n",
    "        if hash_cell_2 not in H_good_2:\n",
    "            continue\n",
    "         \n",
    "        hash_cell_3 = hash(key) % max_hash3\n",
    "        if hash_cell_3 not in H_good_3:\n",
    "            continue\n",
    "\n",
    "        hash_cell_4 = hash(key) % max_hash4\n",
    "        if hash_cell_4 not in H_good_4:\n",
    "            continue\n",
    "        \n",
    "        hash_cell_5 = hash(key) % max_hash5\n",
    "        if hash_cell_5 not in H_good_5:\n",
    "            continue\n",
    "        \n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "        \n",
    "    #print(\"{} items\".format(len(C2)))\n",
    "\n",
    "   \n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= N:\n",
    "            L2[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L2), N))\n",
    "    \n",
    "    # append results\n",
    "    s_list.append(s)\n",
    "    L2_counts_list.append(len(L2))\n",
    "    Wall_time_list_ns.append(time.time() - start)\n",
    "    return\n",
    "#=================================================\n",
    "\n",
    "# Lets modify our A-priori function for frequent 3 tuple \n",
    "\n",
    "def apriori_3tuple(s,k1,k2,k3):\n",
    "    \n",
    "    start = time.time() # start time\n",
    "    N = s\n",
    "    # find frequent individual items\n",
    "    C1 = {}\n",
    "    for key in readdata_hands_on(k1):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "        \n",
    "    #print(\"{} items\".format(len(C1)))\n",
    "    \n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= N:\n",
    "            L1[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L1), N))\n",
    "    \n",
    "    # List comprehensions in python\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()]) \n",
    "    #print(len(C2_items))\n",
    "    \n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata_hands_on(k2):\n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "        \n",
    "    #print(\"{} items\".format(len(C2)))\n",
    "    \n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= N:\n",
    "            L2[key] = count\n",
    "    #print('A-priori: {} items with >{} occurances'.format(len(L2), N))\n",
    "    \n",
    "    \n",
    "    L1_items = set(L1.keys())\n",
    "    L2_items = set(L2.keys())\n",
    "    \n",
    "    # SLOW! (too many possible 3-tuples) So let's be smart and use some time constrain.\n",
    "    start = time.time()\n",
    "    PERIOD_OF_TIME = 10\n",
    "\n",
    "    # find frequent 3-tuples\n",
    "    C3 = {}\n",
    "    for key in readdata_hands_on2(k3):\n",
    "        \n",
    "        # filter out non-frequent tuples\n",
    "        # A-Priori filtering, option 2: generate all possible subsets and check that they all are frequent\n",
    "        non_freq_1 = set([frozenset(x) for x in itertools.combinations(list(key), 1)]) - L1_items\n",
    "        if len(non_freq_1) > 0:\n",
    "            continue\n",
    "\n",
    "        non_freq_2 = set([frozenset(x) for x in itertools.combinations(list(key), 2)]) - L2_items\n",
    "        if len(non_freq_2) > 0:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C3:\n",
    "            C3[key] = 1\n",
    "        else:\n",
    "            C3[key] += 1\n",
    "    \n",
    "        ################################\n",
    "        # break out of a slow function #\n",
    "        if time.time() > start + PERIOD_OF_TIME : \n",
    "            print('Time is running out')\n",
    "            break\n",
    "        \n",
    "    #print(\"{} items\".format(len(C3)))\n",
    "    \n",
    "    # filter stage\n",
    "    L3 = {}\n",
    "    for key, count in C3.items():\n",
    "        if count >= N:\n",
    "            L3[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L3), Ntest))\n",
    "    \n",
    "    # append results\n",
    "    s_list.append(s)\n",
    "    L3_counts_list.append(len(L3))\n",
    "    Wall_time_list_ns.append(time.time() - start)\n",
    "    \n",
    "    \n",
    "    return\n",
    "#=================================================\n",
    "\n",
    "# Lets modify our A-priori function for frequent 4 tuple\n",
    "\n",
    "def apriori_4tuple(s,k1,k2,k3,k4):\n",
    "    \n",
    "    start = time.time() # start time\n",
    "    N = s\n",
    "    # find frequent individual items\n",
    "    C1 = {}\n",
    "    for key in readdata_hands_on(k1):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "        \n",
    "    #print(\"{} items\".format(len(C1)))\n",
    "    \n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= N:\n",
    "            L1[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L1), N))\n",
    "    \n",
    "    # List comprehensions in python\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()]) \n",
    "    #print(len(C2_items))\n",
    "    \n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata_hands_on(k2):\n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "        \n",
    "    #print(\"{} items\".format(len(C2)))\n",
    "    \n",
    "    # filter stage\n",
    "    L2 = {}\n",
    "    for key, count in C2.items():\n",
    "        if count >= N:\n",
    "            L2[key] = count\n",
    "    #print('A-priori: {} items with >{} occurances'.format(len(L2), N))\n",
    "    \n",
    "    \n",
    "    L1_items = set(L1.keys())\n",
    "    L2_items = set(L2.keys())\n",
    "    \n",
    "    # SLOW! (too many possible 3-tuples) So let's be smart and use some time constrain.\n",
    "    start = time.time()\n",
    "    PERIOD_OF_TIME = 10\n",
    "\n",
    "    # find frequent 3-tuples\n",
    "    C3 = {}\n",
    "    for key in readdata_hands_on2(k3):\n",
    "        \n",
    "        # filter out non-frequent tuples\n",
    "        # A-Priori filtering, option 2: generate all possible subsets and check that they all are frequent\n",
    "        non_freq_1 = set([frozenset(x) for x in itertools.combinations(list(key), 1)]) - L1_items\n",
    "        if len(non_freq_1) > 0:\n",
    "            continue\n",
    "\n",
    "        non_freq_2 = set([frozenset(x) for x in itertools.combinations(list(key), 2)]) - L2_items\n",
    "        if len(non_freq_2) > 0:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C3:\n",
    "            C3[key] = 1\n",
    "        else:\n",
    "            C3[key] += 1\n",
    "    \n",
    "        ################################\n",
    "        # break out of a slow function #\n",
    "        if time.time() > start + PERIOD_OF_TIME : \n",
    "            print('Time is running out for freq 3-tuple')\n",
    "            break\n",
    "        \n",
    "    #print(\"{} items\".format(len(C3)))\n",
    "    \n",
    "    # filter stage\n",
    "    L3 = {}\n",
    "    for key, count in C3.items():\n",
    "        if count >= N:\n",
    "            L3[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L3), Ntest))\n",
    "    \n",
    "    \n",
    "    # find frequent 4 tuple ==================================\n",
    "    L3_items = set(L3.keys())\n",
    "    \n",
    "    # SLOW! (too many possible 4-tuples) So let's be smart and use some time constrain.\n",
    "    start = time.time()\n",
    "    PERIOD_OF_TIME = 10  \n",
    "\n",
    "    # find frequent 4-tuples\n",
    "    C4 = {}\n",
    "    for key in readdata_hands_on2(k4):\n",
    "        \n",
    "        # filter out non-frequent tuples\n",
    "        # A-Priori filtering, option 2: generate all possible subsets and check that they all are frequent\n",
    "        non_freq_1 = set([frozenset(x) for x in itertools.combinations(list(key), 1)]) - L1_items\n",
    "        if len(non_freq_1) > 0:\n",
    "            continue\n",
    "\n",
    "        non_freq_2 = set([frozenset(x) for x in itertools.combinations(list(key), 2)]) - L2_items\n",
    "        if len(non_freq_2) > 0:\n",
    "            continue\n",
    "\n",
    "        non_freq_3 = set([frozenset(x) for x in itertools.combinations(list(key), 3)]) - L3_items\n",
    "        if len(non_freq_3) > 0:\n",
    "            continue   \n",
    "            \n",
    "            \n",
    "        # record frequent tuples\n",
    "        if key not in C4:\n",
    "            C4[key] = 1\n",
    "        else:\n",
    "            C4[key] += 1\n",
    "    \n",
    "        ################################\n",
    "        # break out of a slow function #\n",
    "        if time.time() > start + PERIOD_OF_TIME : \n",
    "            print('Time is running out for freq 4-tuple')\n",
    "            break\n",
    "        \n",
    "    #print(\"{} items\".format(len(C4)))\n",
    "    \n",
    "    # filter stage\n",
    "    L4 = {}\n",
    "    for key, count in C4.items():\n",
    "        if count >= N:\n",
    "            L4[key] = count\n",
    "    #print('{} items with >{} occurances'.format(len(L4), N))\n",
    "    \n",
    "\n",
    "    \n",
    "    # append results\n",
    "    s_list.append(s)\n",
    "    L4_counts_list.append(len(L4))\n",
    "    Wall_time_list_ns.append(time.time() - start)\n",
    "    \n",
    "    \n",
    "    return \n",
    "#=================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find the frequent pair of items (2-tuples) using the naïve, A-priori and PCY algorithms. For each of these compare the time of execution and results for supports s=10, 50, 100. Comment your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anthology+abstracts.bib.gz'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1[0] #this is the bib compressed file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking our already downloaded anthology+abstracts.bib.gz file\n",
    "import gzip \n",
    "import shutil\n",
    "with gzip.open(r1[0], 'rb') as f_in:\n",
    "    with open('anthology.bib', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bibtexparser\n",
      "  Downloading bibtexparser-1.2.0.tar.gz (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from bibtexparser) (2.4.7)\n",
      "Collecting future>=0.16.0\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 2.7 MB/s eta 0:00:01�███████████████████▊      | 665 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: bibtexparser, future\n",
      "  Building wheel for bibtexparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bibtexparser: filename=bibtexparser-1.2.0-py3-none-any.whl size=36712 sha256=2231d9cfd623932f56b4943114d990ff760f85298d8e5e0a51345ebd01b09c9e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/3e/13/1d/09c37a40f39ddd7b226719a797f1896a5b95d730de27e7a505\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=849438fdd4546e8f3ce6cc877ce49f715b081e3d08e27084ff6071dfb8bfb7ca\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "Successfully built bibtexparser future\n",
      "Installing collected packages: future, bibtexparser\n",
      "Successfully installed bibtexparser-1.2.0 future-0.18.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install bibtexparser  # install this is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the authors and editors names\n",
    "# This cell takes about 10 - 13mins to extract\n",
    "\n",
    "import bibtexparser\n",
    "\n",
    "with open('anthology.bib', encoding=\"utf-8\") as bibtex_file:\n",
    "    bib_database = bibtexparser.bparser.BibTexParser(common_strings=True).parse_file(bibtex_file)\n",
    "\n",
    "\n",
    "bib_df = pd.DataFrame(bib_database.entries)\n",
    "\n",
    "author_editor = bib_df[['author', 'editor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>editor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mostafazadeh Davani, Aida  and\\nKiela, Douwe  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singh, Sumer  and\\nLi, Sheng</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hahn, Vanessa  and\\nRuiter, Dana  and\\nKleinba...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Caselli, Tommaso  and\\nBasile, Valerio  and\\nM...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kirk, Hannah  and\\nJun, Yennie  and\\nRauba, Pa...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              author  \\\n",
       "0                                                NaN   \n",
       "1                       Singh, Sumer  and\\nLi, Sheng   \n",
       "2  Hahn, Vanessa  and\\nRuiter, Dana  and\\nKleinba...   \n",
       "3  Caselli, Tommaso  and\\nBasile, Valerio  and\\nM...   \n",
       "4  Kirk, Hannah  and\\nJun, Yennie  and\\nRauba, Pa...   \n",
       "\n",
       "                                              editor  \n",
       "0  Mostafazadeh Davani, Aida  and\\nKiela, Douwe  ...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_editor.head() #sample printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_editor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mostafazadeh Davani, Aida  ;Kiela, Douwe  ;Lam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singh, Sumer  ;Li, Sheng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hahn, Vanessa  ;Ruiter, Dana  ;Kleinbauer, Tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Caselli, Tommaso  ;Basile, Valerio  ;Mitrovi{\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kirk, Hannah  ;Jun, Yennie  ;Rauba, Paulius  ;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       author_editor\n",
       "0  Mostafazadeh Davani, Aida  ;Kiela, Douwe  ;Lam...\n",
       "1                           Singh, Sumer  ;Li, Sheng\n",
       "2  Hahn, Vanessa  ;Ruiter, Dana  ;Kleinbauer, Tho...\n",
       "3  Caselli, Tommaso  ;Basile, Valerio  ;Mitrovi{\\...\n",
       "4  Kirk, Hannah  ;Jun, Yennie  ;Rauba, Paulius  ;..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Lets pre-process\n",
    "\n",
    "# make a copy of it\n",
    "aut_edit = author_editor.copy() \n",
    "\n",
    "# create a new column\n",
    "aut_edit['author_editor'] = 'a'\n",
    "\n",
    "\n",
    "# populate this new column with values from the original columns\n",
    "for i in range(len(aut_edit)):\n",
    "    if pd.isna(aut_edit.iloc[i,0]):\n",
    "        aut_edit.iloc[i,2] = aut_edit.iloc[i,1]\n",
    "        \n",
    "    else:\n",
    "        aut_edit.iloc[i,2] = aut_edit.iloc[i,0]\n",
    "        \n",
    "\n",
    "        \n",
    "# drop old columns\n",
    "aut_edit.drop(labels=['author','editor'], axis=1, inplace=True)\n",
    "\n",
    "# drop na\n",
    "aut_edit = aut_edit.dropna()\n",
    "\n",
    "\n",
    "# Replace and\\n with ; (which separates each author's/editor's full name)\n",
    "aut_edit['author_editor'] = aut_edit['author_editor'].str.replace('and\\n',';')\n",
    "\n",
    "\n",
    "# see the new head\n",
    "aut_edit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy \n",
    "\n",
    "aut_edit.to_csv('anthology_authors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the frequent pair of items (2-tuples) using the naïve algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aut_edit = pd.read_csv('anthology_authors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73197"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbaskets = aut_edit.shape[0]\n",
    "nbaskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing out list \n",
    "s_list = []\n",
    "L2_counts_list = []\n",
    "Wall_time_list_ns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "###### for supports s=10\n",
    "\n",
    "s = 10\n",
    "naive_items(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "###### for supports s=50\n",
    "\n",
    "s = 50\n",
    "naive_items(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "###### for supports s=100\n",
    "\n",
    "s = 100\n",
    "naive_items(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive algorithm result\n",
    "dict = {'s': s_list, 'lenL2': L2_counts_list, 'wall_time(s)': Wall_time_list_ns} \n",
    "naive_algo = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the frequent pair of items (2-tuples) using the A-priori  algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing out list \n",
    "s_list = []\n",
    "L2_counts_list = []\n",
    "Wall_time_list_ns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "###### for supports s=10\n",
    "\n",
    "s = 10\n",
    "apriori_items(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "###### for supports s=50\n",
    "\n",
    "s = 50\n",
    "apriori_items(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "###### for supports s=100\n",
    "\n",
    "s = 100\n",
    "apriori_items(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apriori algorithm result\n",
    "dict = {'s': s_list, 'lenL2': L2_counts_list, 'wall_time(s)': Wall_time_list_ns} \n",
    "apriori_algo = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the frequent pair of items (2-tuples) using the PCY algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing out list \n",
    "s_list = []\n",
    "L2_counts_list = []\n",
    "Wall_time_list_ns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "###### for supports s=10\n",
    "###### use zzz to collect the un-needed output(the print out was so much)\n",
    "s = 10\n",
    "zzz = pcy_items(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset({'Sumita, Eiichiro\"', 'Utiyama, Masao  '}): 56,\n",
       " frozenset({'Bhattacharyya, Pushpak\"', 'Ekbal, Asif  '}): 57}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####\n",
    "###### for supports s=50\n",
    "\n",
    "s = 50\n",
    "pcy_items(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####\n",
    "###### for supports s=100\n",
    "\n",
    "s = 100\n",
    "pcy_items(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcy algorithm result\n",
    "dict = {'s': s_list, 'lenL2': L2_counts_list, 'wall_time(s)': Wall_time_list_ns} \n",
    "pcy_algo = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number1 - Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Algorithm results\n",
      "     s  lenL2  wall_time(s)\n",
      "0   10    428      1.963969\n",
      "1   50      2      1.730492\n",
      "2  100      0      1.516440\n",
      "\n",
      "A-priori Algorithm results\n",
      "     s  lenL2  wall_time(s)\n",
      "0   10    428     17.699204\n",
      "1   50      2      0.678411\n",
      "2  100      0      0.627381\n",
      "\n",
      "PCY Algorithm results\n",
      "     s  lenL2  wall_time(s)\n",
      "0   10    428     19.636965\n",
      "1   50      2      1.558505\n",
      "2  100      0      1.460125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Naive Algorithm results'); print(naive_algo); print(''); print('A-priori Algorithm results'); print(apriori_algo); print(''); print('PCY Algorithm results'); print(pcy_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COMMENT\n",
    "- They all had the same number of items occurences greater than each support thresholds\n",
    "- However for support threshold 10, naive algorithm was the fastest while pcy was the slowest.\n",
    "- For higher support thresholds the speed of execution of apriori and pcy were faster than that of naive algorithm; but in this instance, apriori algorithm carried the day as per speed. If we were to factor in memory usage, pcy will be the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For the PCY algorithm, create up to 5 compact hash tables. What is the difference in results and time of execution for 1,2,3,4 and 5 tables? Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "slist = [10, 50, 100] # various support thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing out list \n",
    "s_list = []\n",
    "L2_counts_list = []\n",
    "Wall_time_list_ns = []\n",
    "\n",
    "#looping over the various support threshholds\n",
    "for x in range(len(slist)):\n",
    "    oneHashTablePCY(slist[x])\n",
    "\n",
    "#  pcy 1 compact hash table result\n",
    "dict = {'s': s_list, 'lenL2': L2_counts_list, 'wall_time(s)': Wall_time_list_ns} \n",
    "pcy_1HashTable = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing out list \n",
    "s_list = []\n",
    "L2_counts_list = []\n",
    "Wall_time_list_ns = []\n",
    "\n",
    "#looping over the various support threshholds\n",
    "for x in range(len(slist)):\n",
    "    twoHashTablePCY(slist[x])\n",
    "\n",
    "#  pcy 2 compact hash tables result\n",
    "dict = {'s': s_list, 'lenL2': L2_counts_list, 'wall_time(s)': Wall_time_list_ns} \n",
    "pcy_2HashTable = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing out list \n",
    "s_list = []\n",
    "L2_counts_list = []\n",
    "Wall_time_list_ns = []\n",
    "\n",
    "#looping over the various support threshholds\n",
    "for x in range(len(slist)):\n",
    "    threeHashTablePCY(slist[x])\n",
    "\n",
    "# pcy 3 compact hash tables result\n",
    "dict = {'s': s_list, 'lenL2': L2_counts_list, 'wall_time(s)': Wall_time_list_ns} \n",
    "pcy_3HashTable = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing out list \n",
    "s_list = []\n",
    "L2_counts_list = []\n",
    "Wall_time_list_ns = []\n",
    "\n",
    "#looping over the various support threshholds\n",
    "for x in range(len(slist)):\n",
    "    fourHashTablePCY(slist[x])\n",
    "\n",
    "# pcy 4 compact hash tables result\n",
    "dict = {'s': s_list, 'lenL2': L2_counts_list, 'wall_time(s)': Wall_time_list_ns} \n",
    "pcy_4HashTable = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing out list \n",
    "s_list = []\n",
    "L2_counts_list = []\n",
    "Wall_time_list_ns = []\n",
    "\n",
    "#looping over the various support threshholds\n",
    "for x in range(len(slist)):\n",
    "    fiveHashTablePCY(slist[x])\n",
    "\n",
    "# pcy 5 compact hash tables result\n",
    "dict = {'s': s_list, 'lenL2': L2_counts_list, 'wall_time(s)': Wall_time_list_ns} \n",
    "pcy_5HashTable = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number 2 - Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcy 1 compact hash tables result\n",
      "     s  lenL2  wall_time(s)\n",
      "0   10    428     17.723017\n",
      "1   50      2      1.241099\n",
      "2  100      0      1.202283\n",
      "\n",
      "pcy 2 compact hash tables result\n",
      "     s  lenL2  wall_time(s)\n",
      "0   10    428     16.029233\n",
      "1   50      2      1.519577\n",
      "2  100      0      1.772187\n",
      "\n",
      "pcy 3 compact hash tables result\n",
      "     s  lenL2  wall_time(s)\n",
      "0   10    428     16.863230\n",
      "1   50      2      1.930255\n",
      "2  100      0      1.855751\n",
      "\n",
      "pcy 4 compact hash tables result\n",
      "     s  lenL2  wall_time(s)\n",
      "0   10    428     19.034633\n",
      "1   50      2      2.294307\n",
      "2  100      0      2.186956\n",
      "\n",
      "pcy 5 compact hash tables result\n",
      "     s  lenL2  wall_time(s)\n",
      "0   10    428     20.163187\n",
      "1   50      2      2.606501\n",
      "2  100      0      2.651404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('pcy 1 compact hash tables result'); print(pcy_1HashTable);print('');\n",
    "print('pcy 2 compact hash tables result'); print(pcy_2HashTable);print('');\n",
    "print('pcy 3 compact hash tables result'); print(pcy_3HashTable);print('');\n",
    "print('pcy 4 compact hash tables result'); print(pcy_4HashTable);print('');\n",
    "print('pcy 5 compact hash tables result'); print(pcy_5HashTable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COMMENT\n",
    "- The execution times of support threshold 10 as the compact hashing tables increases, overall, seems to be fairly consistent\n",
    "- But the other support thresholds shows clearly that the times are increasing, showing that more hashing is been done which should lead to a better memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find the final list of k-frequent items (k-tuples) for k=3 and 4. Experiment a bit and describe the best value for the support in each case. Warning: You can use any of the three algorithms, but be careful, because the algorithm can take too long if you don't chose it properly (well, basically don't use the naïve approach ;)).\n",
    "\n",
    "The for loop cells will take some time to spin up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing out list \n",
    "s_list = []\n",
    "#L2_counts_list = []\n",
    "L3_counts_list = []\n",
    "Wall_time_list_ns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping over the various support threshholds\n",
    "\n",
    "\n",
    "for x in range(len(slist)):\n",
    "    apriori_3tuple(slist[x],k1=1, k2=2, k3=3)\n",
    "\n",
    "# result\n",
    "dict = {'s': s_list, 'lenL3': L3_counts_list, 'wall_time(s)': Wall_time_list_ns} \n",
    "pcy_3tuple = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing out list \n",
    "s_list = []\n",
    "L4_counts_list = []\n",
    "Wall_time_list_ns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time is running out for freq 4-tuple\n",
      "Time is running out for freq 4-tuple\n"
     ]
    }
   ],
   "source": [
    "#looping over the various support threshholds\n",
    "\n",
    "slist2 = [8,9]\n",
    "\n",
    "for x in range(len(slist2)):\n",
    "    apriori_4tuple(slist2[x],k1=1, k2=2, k3=3, k4=4)\n",
    "\n",
    "# result\n",
    "dict = {'s': s_list, 'lenL4': L4_counts_list, 'wall_time(s)': Wall_time_list_ns} \n",
    "pcy_4tuple = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number 3 - Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-tuple results\n",
      "     s  lenL3  wall_time(s)\n",
      "0   10    109      2.315736\n",
      "1   50      0      2.405657\n",
      "2  100      0      2.130113\n",
      "\n",
      "4-tuple results\n",
      "   s  lenL4  wall_time(s)\n",
      "0  8     15     10.105361\n",
      "1  9     38     12.598465\n"
     ]
    }
   ],
   "source": [
    "print('3-tuple results'); print(pcy_3tuple);print('');\n",
    "print('4-tuple results'); print(pcy_4tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COMMENT\n",
    "- 3-tuples started appearing for s values from 10\n",
    "- And the 4-tuples started appearing for s values 9 and below\n",
    "- Notable is the time, 3-tuples operations were faster, because the hashing tables were less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Using one of the results of the previous items, for one k (k=2 or 3) find the possible clusters using the 1-NN criteria. Comment your results.\n",
    "\n",
    "1-NN means that if you have a tuple {A,B,C} and {C,E,F} then because they share one element {C}, then they belong to the same cluster {A,B,C,E,F}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are dealing here with K=1-Nearest Neighbor\n",
    "\n",
    "# We need to collect the L2 results from our support threshold s, 10\n",
    "# and 50 using our earlier function for 2-tuple pcy\n",
    "s_list = []\n",
    "L2_counts_list = []\n",
    "Wall_time_list_ns = []\n",
    "\n",
    "L2_s10 = pcy_items(10)\n",
    "L2_s50 = pcy_items(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted frozensets to list\n",
    "L2_s10_list = []\n",
    "L2_s10_list.append([list(x) for x in L2_s10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Utiyama, Masao  ', 'Sumita, Eiichiro\"'],\n",
       " ['Sumita, Eiichiro\"', 'Watanabe, Taro  '],\n",
       " ['Federmann, Christian  ', 'Chatterjee, Rajen  '],\n",
       " ['Graham, Yvette  ', 'Chatterjee, Rajen  '],\n",
       " ['Haddow, Barry  ', 'Chatterjee, Rajen  ']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frozensets converted to list, we print 5 out of 426\n",
    "L2_s10_list[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grp1 = []\n",
    "#i = 0\n",
    "#for x in range(len(L2_s10_list[0])):\n",
    "#    if i == 20:\n",
    "#        break\n",
    "        \n",
    "#    else:\n",
    "        \n",
    "#        count = []\n",
    "#        names = []\n",
    "#        for y in range(len(L2_s10_list[0]) - 1):\n",
    "#            for item in range(len(L2_s10_list[0][y])):\n",
    "#                if (L2_s10_list[0][x][0]) == (L2_s10_list[0][y][item]):\n",
    "#                    count.append(y)\n",
    "#                    #names1.append(L2_s10_list[0][y])\n",
    "#            print(count)\n",
    "#            grp1.append(count)\n",
    "#            #print(names)\n",
    "#            #i += 1\n",
    "        \n",
    "\n",
    "###=================== Extracting the Clusters\n",
    "### I tested with 20 samples as in the above, before applying to the whole dataset\n",
    "\n",
    "grp1 = []\n",
    "i = 0\n",
    "for x in range(len(L2_s10_list[0])):  \n",
    "    count = []\n",
    "    names = []\n",
    "    for y in range(len(L2_s10_list[0]) - 1):\n",
    "        for item in range(len(L2_s10_list[0][y])):\n",
    "            if (L2_s10_list[0][x][0]) == (L2_s10_list[0][y][item]):\n",
    "                count.append(y)\n",
    "                #names1.append(L2_s10_list[0][y])\n",
    "        #print(count)\n",
    "        grp1.append(count)\n",
    "        #print(names)\n",
    "        #i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This filters the list and remove duplicates (this process took a bit of time)\n",
    "\n",
    "final_grp = [i for j, i in enumerate(grp1) if i not in grp1[:j]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 35, 117, 118, 352],\n",
       " [0, 1, 353],\n",
       " [2, 8, 9, 10, 11, 12, 24, 25, 45],\n",
       " [3, 8, 13, 14, 15, 26],\n",
       " [4, 9, 13, 16, 17, 18, 27, 28, 252, 276, 306],\n",
       " [5, 10, 14, 16, 19, 20, 29, 30, 253],\n",
       " [6, 11, 17, 19, 21],\n",
       " [7, 12, 15, 18, 20, 21, 31, 32, 254, 342],\n",
       " [22, 24, 26, 27, 29, 31, 33, 255],\n",
       " [23, 25, 28, 30, 32, 33, 34, 63, 291, 304],\n",
       " [34],\n",
       " [36, 39, 160],\n",
       " [36, 37, 167],\n",
       " [38, 158, 159, 182],\n",
       " [40],\n",
       " [41],\n",
       " [42],\n",
       " [43]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_grp[:18] #Lets see the first 18 items out of 289"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This for loop counts and prints out the clusters(if both print\n",
    "# statements are uncommented)\n",
    "\n",
    "clusters_names = []\n",
    "clusters_names_counter = 0\n",
    "for i in range(len(final_grp)):\n",
    "    if len(final_grp[i]) > 1: # this makes such the cluster must be at least 2 data points\n",
    "        for j in range(len(final_grp[i])):\n",
    "            clusters_names.append(L2_s10_list[0][final_grp[i][j]])\n",
    "        #print(L2_s10_list[0][final_grp[i][j]])\n",
    "\n",
    "        #print(\"\")  \n",
    "        clusters_names_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number 4 - Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 121 clusters\n"
     ]
    }
   ],
   "source": [
    "# print out cluster number\n",
    "print (\"There are {} clusters\".format(clusters_names_counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Utiyama, Masao  ', 'Sumita, Eiichiro\"']\n",
      "['Utiyama, Masao  ', 'Sumita, Eiichiro  ']\n",
      "['Utiyama, Masao  ', 'Wang, Rui  ']\n",
      "['Chen, Kehai  ', 'Utiyama, Masao  ']\n",
      "['Finch, Andrew  ', 'Utiyama, Masao  ']\n",
      "\n",
      "['Utiyama, Masao  ', 'Sumita, Eiichiro\"']\n",
      "['Sumita, Eiichiro\"', 'Watanabe, Taro  ']\n",
      "['Finch, Andrew  ', 'Sumita, Eiichiro\"']\n",
      "\n",
      "['Federmann, Christian  ', 'Chatterjee, Rajen  ']\n",
      "['Graham, Yvette  ', 'Federmann, Christian  ']\n",
      "['Federmann, Christian  ', 'Haddow, Barry  ']\n",
      "['Federmann, Christian  ', 'Huck, Matthias  ']\n",
      "['Yepes, Antonio Jimeno  ', 'Federmann, Christian  ']\n",
      "['Koehn, Philipp  ', 'Federmann, Christian  ']\n",
      "['Monz, Christof  ', 'Federmann, Christian  ']\n",
      "['Federmann, Christian  ', 'Negri, Matteo  ']\n",
      "['Post, Matt  ', 'Federmann, Christian  ']\n",
      "\n",
      "['Graham, Yvette  ', 'Chatterjee, Rajen  ']\n",
      "['Graham, Yvette  ', 'Federmann, Christian  ']\n",
      "['Graham, Yvette  ', 'Haddow, Barry  ']\n",
      "['Graham, Yvette  ', 'Huck, Matthias  ']\n",
      "['Koehn, Philipp  ', 'Graham, Yvette  ']\n",
      "['Graham, Yvette  ', 'Monz, Christof  ']\n",
      "\n",
      "['Haddow, Barry  ', 'Chatterjee, Rajen  ']\n",
      "['Federmann, Christian  ', 'Haddow, Barry  ']\n",
      "['Graham, Yvette  ', 'Haddow, Barry  ']\n",
      "['Huck, Matthias  ', 'Haddow, Barry  ']\n",
      "['Yepes, Antonio Jimeno  ', 'Haddow, Barry  ']\n",
      "['Koehn, Philipp  ', 'Haddow, Barry  ']\n",
      "['Monz, Christof  ', 'Haddow, Barry  ']\n",
      "['Negri, Matteo  ', 'Haddow, Barry  ']\n",
      "['Post, Matt  ', 'Haddow, Barry  ']\n",
      "['Birch, Alexandra\"', 'Haddow, Barry  ']\n",
      "['Specia, Lucia  ', 'Haddow, Barry  ']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let print out the first 5 clusters\n",
    "i = 0\n",
    "clusters_names = []\n",
    "clusters_names_counter = 0\n",
    "for i in range(len(final_grp)):\n",
    "    for j in range(len(final_grp[i])):\n",
    "        clusters_names.append(L2_s10_list[0][final_grp[i][j]])\n",
    "        print(L2_s10_list[0][final_grp[i][j]])\n",
    "\n",
    "    print(\"\")  \n",
    "    clusters_names_counter += 1\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENTS\n",
    "- We had about 121 clusters after filtering out some disparsed node, which made it 289 (within these cluster there could be major or massive ones that other methods could build categories as cluster1, cluster2 and so on)\n",
    "- 1-NN yielded clusters with upto 11 memebers(and could even be more if sorted), this method is more effective and clearer than the one used for the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
